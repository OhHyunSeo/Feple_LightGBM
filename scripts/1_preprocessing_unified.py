# -*- coding: utf-8 -*-
"""
1_preprocessing_unified.py
- í†µí•© data í´ë”ì—ì„œ íŒŒì¼ëª… ê¸°ë°˜ìœ¼ë¡œ ë¶„ë¥˜/ìš”ì•½/ì§ˆì˜ì‘ë‹µ ì²˜ë¦¬
- íŒŒì¼ëª… íŒ¨í„´: ë¶„ë¥˜_ì„¸ì…˜ID_ë²ˆí˜¸.json, ìš”ì•½_ì„¸ì…˜ID_ë²ˆí˜¸.json, ì§ˆì˜ì‘ë‹µ_ì„¸ì…˜ID_ë²ˆí˜¸.json
"""

import os
import glob
import json
from collections import defaultdict
from tqdm import tqdm

class UnifiedPreprocessor:
    """í†µí•© ì „ì²˜ë¦¬ê¸°"""
    
    def __init__(self, input_dir="data", output_base="json_merge"):
        self.input_dir = input_dir
        self.output_base = output_base
        
        # ì¶œë ¥ í´ë” ì„¤ì •
        self.output_dirs = {
            'ë¶„ë¥˜': f'{output_base}/classification_merge_output',
            'ìš”ì•½': f'{output_base}/summary_merge_output', 
            'ì§ˆì˜ì‘ë‹µ': f'{output_base}/qa_merge_output'
        }
        
        # ì¶œë ¥ í´ë” ìƒì„±
        for out_dir in self.output_dirs.values():
            os.makedirs(out_dir, exist_ok=True)
        
        # íŒŒì¼ëª… íŒ¨í„´ë³„ ë§¤í•‘
        self.type_mappings = {
            'ë¶„ë¥˜': ['ë¶„ë¥˜', 'classification', 'class'],
            'ìš”ì•½': ['ìš”ì•½', 'summary', 'sum'],
            'ì§ˆì˜ì‘ë‹µ': ['ì§ˆì˜ì‘ë‹µ', 'qa', 'qna', 'question']
        }
    
    def detect_file_type(self, filename):
        """íŒŒì¼ëª…ì—ì„œ ë°ì´í„° íƒ€ì… ê°ì§€"""
        filename_lower = filename.lower()
        
        for data_type, keywords in self.type_mappings.items():
            for keyword in keywords:
                if keyword in filename_lower:
                    return data_type
        
        return None
    
    def extract_session_id(self, filename):
        """íŒŒì¼ëª…ì—ì„œ ì„¸ì…˜ ID ì¶”ì¶œ"""
        # íŒŒì¼ëª… íŒ¨í„´: ë¶„ë¥˜_ì„¸ì…˜ID_ë²ˆí˜¸.json ë˜ëŠ” ì„¸ì…˜ID_ë¶„ë¥˜_ë²ˆí˜¸.json ë“±
        parts = filename.replace('.json', '').split('_')
        
        # ìˆ«ìë¡œë§Œ ì´ë£¨ì–´ì§„ ë¶€ë¶„ì„ ì„¸ì…˜ IDë¡œ ê°„ì£¼
        for part in parts:
            if part.isdigit():
                return part
        
        # ìˆ«ìê°€ ì—†ìœ¼ë©´ íŒŒì¼ëª… ì „ì²´ì—ì„œ ìˆ«ì ì¶”ì¶œ
        import re
        numbers = re.findall(r'\d+', filename)
        if numbers:
            return numbers[0]  # ì²« ë²ˆì§¸ ìˆ«ìë¥¼ ì„¸ì…˜ IDë¡œ ì‚¬ìš©
        
        return 'unknown'
    
    def extract_file_number(self, filename):
        """íŒŒì¼ëª…ì—ì„œ íŒŒì¼ ë²ˆí˜¸ ì¶”ì¶œ (ì •ë ¬ìš©)"""
        import re
        numbers = re.findall(r'\d+', filename)
        if len(numbers) >= 2:
            return int(numbers[-1])  # ë§ˆì§€ë§‰ ìˆ«ìë¥¼ íŒŒì¼ ë²ˆí˜¸ë¡œ ì‚¬ìš©
        elif len(numbers) == 1:
            return int(numbers[0])
        return 0
    
    def group_files_by_type_and_session(self):
        """íŒŒì¼ë“¤ì„ íƒ€ì…ê³¼ ì„¸ì…˜ë³„ë¡œ ê·¸ë£¹í™”"""
        print(f"ğŸ“ ì…ë ¥ í´ë” ìŠ¤ìº”: {self.input_dir}")
        
        grouped = {
            'ë¶„ë¥˜': defaultdict(list),
            'ìš”ì•½': defaultdict(list),
            'ì§ˆì˜ì‘ë‹µ': defaultdict(list)
        }
        
        # JSON íŒŒì¼ ìŠ¤ìº”
        pattern = os.path.join(self.input_dir, '*.json')
        all_files = glob.glob(pattern)
        
        print(f"ğŸ“„ ë°œê²¬ëœ JSON íŒŒì¼: {len(all_files)}ê°œ")
        
        for filepath in all_files:
            filename = os.path.basename(filepath)
            
            # íŒŒì¼ íƒ€ì… ê°ì§€
            file_type = self.detect_file_type(filename)
            if not file_type:
                print(f"âš ï¸ íƒ€ì…ì„ ê°ì§€í•  ìˆ˜ ì—†ëŠ” íŒŒì¼: {filename}")
                continue
            
            # ì„¸ì…˜ ID ì¶”ì¶œ
            session_id = self.extract_session_id(filename)
            
            # ê·¸ë£¹ì— ì¶”ê°€
            grouped[file_type][session_id].append(filepath)
            
            print(f"âœ… {file_type} | ì„¸ì…˜ {session_id} | {filename}")
        
        # ê·¸ë£¹í™” ê²°ê³¼ ì¶œë ¥
        for data_type, sessions in grouped.items():
            print(f"\nğŸ“Š {data_type}: {len(sessions)}ê°œ ì„¸ì…˜")
            for session_id, files in sessions.items():
                print(f"   ì„¸ì…˜ {session_id}: {len(files)}ê°œ íŒŒì¼")
        
        return grouped
    
    def process_classification_files(self, session_files):
        """ë¶„ë¥˜ íŒŒì¼ë“¤ ì²˜ë¦¬"""
        merged_data = []
        
        for session_id, files in session_files.items():
            if len(files) < 1:
                continue
            
            # íŒŒì¼ ë²ˆí˜¸ ìˆœì„œëŒ€ë¡œ ì •ë ¬
            ordered_files = sorted(files, key=lambda x: self.extract_file_number(os.path.basename(x)))
            
            # consulting_content ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ë³‘í•©
            content_groups = {}
            
            for filepath in ordered_files:
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œ ì‚¬ìš©
                    if isinstance(data, list):
                        data = data[0]
                    
                    content = data.get('consulting_content', '')
                    
                    if content not in content_groups:
                        # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì €ì¥ (instructions, input ì œì™¸)
                        base_data = {k: v for k, v in data.items() 
                                   if k not in ('instructions', 'input')}
                        content_groups[content] = {
                            'base': base_data,
                            'data': []
                        }
                    
                    # instructions ë°ì´í„° ë³‘í•©
                    instructions = data.get('instructions', [])
                    if instructions:
                        instruction_data = instructions[0].get('data', [])
                        content_groups[content]['data'].extend(instruction_data)
                
                except Exception as e:
                    print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {filepath}: {e}")
            
            # ìµœì¢… ê°ì²´ ìƒì„±
            for content, info in content_groups.items():
                obj = info['base'].copy()
                obj['consulting_content'] = content
                obj['instructions'] = [{
                    'tuning_type': 'ë¶„ë¥˜',
                    'data': info['data']
                }]
                merged_data.append(obj)
            
            # ì„¸ì…˜ë³„ íŒŒì¼ ì €ì¥
            output = {
                'session_id': session_id,
                'ë¶„ë¥˜': merged_data
            }
            
            output_path = os.path.join(
                self.output_dirs['ë¶„ë¥˜'], 
                f'merged_classification_{session_id}_final.json'
            )
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(output, f, ensure_ascii=False, indent=2)
            
            merged_data.clear()  # ë‹¤ìŒ ì„¸ì…˜ì„ ìœ„í•´ ì´ˆê¸°í™”
    
    def process_summary_files(self, session_files):
        """ìš”ì•½ íŒŒì¼ë“¤ ì²˜ë¦¬"""
        for session_id, files in session_files.items():
            if len(files) < 1:
                continue
            
            # íŒŒì¼ ë²ˆí˜¸ ìˆœì„œëŒ€ë¡œ ì •ë ¬
            ordered_files = sorted(files, key=lambda x: self.extract_file_number(os.path.basename(x)))
            
            # consulting_content ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ë³‘í•©
            content_groups = {}
            
            for filepath in ordered_files:
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    if isinstance(data, list):
                        data = data[0]
                    
                    content = data.get('consulting_content', '')
                    
                    if content not in content_groups:
                        base_data = {k: v for k, v in data.items() 
                                   if k not in ('instructions', 'input')}
                        content_groups[content] = {
                            'base': base_data,
                            'data': []
                        }
                    
                    instructions = data.get('instructions', [])
                    if instructions:
                        instruction_data = instructions[0].get('data', [])
                        content_groups[content]['data'].extend(instruction_data)
                
                except Exception as e:
                    print(f"âŒ ìš”ì•½ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {filepath}: {e}")
            
            # ìµœì¢… ê°ì²´ ìƒì„±
            merged_data = []
            for content, info in content_groups.items():
                obj = info['base'].copy()
                obj['consulting_content'] = content
                obj['instructions'] = [{
                    'tuning_type': 'ìš”ì•½',
                    'data': info['data']
                }]
                merged_data.append(obj)
            
            # ì„¸ì…˜ë³„ íŒŒì¼ ì €ì¥
            output = {
                'session_id': session_id,
                'ìš”ì•½': merged_data
            }
            
            output_path = os.path.join(
                self.output_dirs['ìš”ì•½'], 
                f'merged_summary_{session_id}.json'
            )
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(output, f, ensure_ascii=False, indent=2)
    
    def process_qa_files(self, session_files):
        """ì§ˆì˜ì‘ë‹µ íŒŒì¼ë“¤ ì²˜ë¦¬"""
        for session_id, files in session_files.items():
            if len(files) < 1:
                continue
            
            # íŒŒì¼ ë²ˆí˜¸ ìˆœì„œëŒ€ë¡œ ì •ë ¬
            ordered_files = sorted(files, key=lambda x: self.extract_file_number(os.path.basename(x)))
            
            # consulting_content ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ë³‘í•©
            content_groups = {}
            
            for filepath in ordered_files:
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    if isinstance(data, list):
                        data = data[0]
                    
                    content = data.get('consulting_content', '')
                    
                    if content not in content_groups:
                        base_data = {k: v for k, v in data.items() 
                                   if k not in ('instructions', 'input')}
                        content_groups[content] = {
                            'base': base_data,
                            'data': []
                        }
                    
                    instructions = data.get('instructions', [])
                    if instructions:
                        instruction_data = instructions[0].get('data', [])
                        content_groups[content]['data'].extend(instruction_data)
                
                except Exception as e:
                    print(f"âŒ ì§ˆì˜ì‘ë‹µ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {filepath}: {e}")
            
            # ìµœì¢… ê°ì²´ ìƒì„±
            merged_data = []
            for content, info in content_groups.items():
                obj = info['base'].copy()
                obj['consulting_content'] = content
                obj['instructions'] = [{
                    'tuning_type': 'ì§ˆì˜ì‘ë‹µ',
                    'data': info['data']
                }]
                merged_data.append(obj)
            
            # ì„¸ì…˜ë³„ íŒŒì¼ ì €ì¥
            output = {
                'session_id': session_id,
                'ì§ˆì˜ì‘ë‹µ': merged_data
            }
            
            output_path = os.path.join(
                self.output_dirs['ì§ˆì˜ì‘ë‹µ'], 
                f'merged_qa_{session_id}.json'
            )
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(output, f, ensure_ascii=False, indent=2)
    
    def remove_input_fields(self):
        """ìƒì„±ëœ ëª¨ë“  ë³‘í•© íŒŒì¼ì—ì„œ input í•„ë“œ ì œê±°"""
        def remove_input_recursive(obj):
            if isinstance(obj, dict):
                obj.pop('input', None)
                for value in obj.values():
                    remove_input_recursive(value)
            elif isinstance(obj, list):
                for item in obj:
                    remove_input_recursive(item)
        
        all_files = []
        for output_dir in self.output_dirs.values():
            all_files.extend(glob.glob(os.path.join(output_dir, '*.json')))
        
        for filepath in tqdm(all_files, desc='Input í•„ë“œ ì œê±° ì¤‘'):
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                remove_input_recursive(data)
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
            
            except Exception as e:
                print(f"âŒ Input ì œê±° ì˜¤ë¥˜ {filepath}: {e}")
        
        print(f"âœ… {len(all_files)}ê°œ íŒŒì¼ì—ì„œ input í•„ë“œ ì œê±° ì™„ë£Œ")
    
    def integrate_all_sessions(self):
        """ëª¨ë“  ì„¸ì…˜ì˜ ë¶„ë¥˜/ìš”ì•½/ì§ˆì˜ì‘ë‹µ ë°ì´í„°ë¥¼ í†µí•©"""
        integration_dir = f'{self.output_base}/integration_data'
        os.makedirs(integration_dir, exist_ok=True)
        
        # ê° ì„¸ì…˜ë³„ë¡œ í†µí•© íŒŒì¼ ìƒì„±
        all_sessions = set()
        
        # ëª¨ë“  ì¶œë ¥ í´ë”ì—ì„œ ì„¸ì…˜ ID ìˆ˜ì§‘
        for data_type, output_dir in self.output_dirs.items():
            for filepath in glob.glob(os.path.join(output_dir, '*.json')):
                filename = os.path.basename(filepath)
                session_id = self.extract_session_id(filename)
                if session_id != 'unknown':
                    all_sessions.add(session_id)
        
        print(f"ğŸ”— {len(all_sessions)}ê°œ ì„¸ì…˜ í†µí•© ì¤‘...")
        
        for session_id in tqdm(all_sessions, desc='ì„¸ì…˜ í†µí•©'):
            integrated_data = {'session_id': session_id}
            
            # ê° ë°ì´í„° íƒ€ì…ë³„ë¡œ íŒŒì¼ ì½ê¸°
            for data_type, output_dir in self.output_dirs.items():
                type_files = glob.glob(os.path.join(output_dir, f'*{session_id}*.json'))
                
                if type_files:
                    try:
                        with open(type_files[0], 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        
                        # ë°ì´í„° íƒ€ì…ë³„ ì •ë³´ ì¶”ê°€
                        if data_type == 'ë¶„ë¥˜':
                            integrated_data['ë¶„ë¥˜'] = data.get('ë¶„ë¥˜', [])
                        elif data_type == 'ìš”ì•½':
                            integrated_data['ìš”ì•½'] = data.get('ìš”ì•½', [])
                        elif data_type == 'ì§ˆì˜ì‘ë‹µ':
                            integrated_data['ì§ˆì˜ì‘ë‹µ'] = data.get('ì§ˆì˜ì‘ë‹µ', [])
                    
                    except Exception as e:
                        print(f"âŒ ì„¸ì…˜ {session_id} {data_type} í†µí•© ì˜¤ë¥˜: {e}")
            
            # í†µí•© íŒŒì¼ ì €ì¥
            output_path = os.path.join(integration_dir, f'final_merged_{session_id}.json')
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(integrated_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ì„¸ì…˜ í†µí•© ì™„ë£Œ â†’ {integration_dir}")
    
    def run(self):
        """ì „ì²´ ì „ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("="*60)
        print("ğŸ”„ í†µí•© ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
        print("="*60)
        
        # 1. íŒŒì¼ ê·¸ë£¹í™”
        grouped_files = self.group_files_by_type_and_session()
        
        # 2. ê° íƒ€ì…ë³„ ì²˜ë¦¬
        print(f"\n[1ë‹¨ê³„] ë¶„ë¥˜ ë°ì´í„° ì²˜ë¦¬...")
        self.process_classification_files(grouped_files['ë¶„ë¥˜'])
        
        print(f"\n[2ë‹¨ê³„] ìš”ì•½ ë°ì´í„° ì²˜ë¦¬...")
        self.process_summary_files(grouped_files['ìš”ì•½'])
        
        print(f"\n[3ë‹¨ê³„] ì§ˆì˜ì‘ë‹µ ë°ì´í„° ì²˜ë¦¬...")
        self.process_qa_files(grouped_files['ì§ˆì˜ì‘ë‹µ'])
        
        # 3. Input í•„ë“œ ì œê±°
        print(f"\n[4ë‹¨ê³„] Input í•„ë“œ ì œê±°...")
        self.remove_input_fields()
        
        # 4. ì„¸ì…˜ í†µí•©
        print(f"\n[5ë‹¨ê³„] ì„¸ì…˜ë³„ ë°ì´í„° í†µí•©...")
        self.integrate_all_sessions()
        
        print("\n" + "="*60)
        print("âœ… í†µí•© ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print("="*60)
        print(f"ğŸ“ ì¶œë ¥ í´ë”:")
        for data_type, output_dir in self.output_dirs.items():
            print(f"   {data_type}: {output_dir}")
        print(f"   í†µí•©: {self.output_base}/integration_data")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ í†µí•© í´ë” ê¸°ë°˜ ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ")
    print("="*50)
    
    # ì…ë ¥ í´ë” í™•ì¸
    input_dir = "data"
    if not os.path.exists(input_dir):
        print(f"âŒ ì…ë ¥ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {input_dir}")
        print("   data í´ë”ë¥¼ ìƒì„±í•˜ê³  JSON íŒŒì¼ë“¤ì„ ë„£ì–´ì£¼ì„¸ìš”.")
        return
    
    # JSON íŒŒì¼ ì¡´ì¬ í™•ì¸
    json_files = glob.glob(os.path.join(input_dir, '*.json'))
    if not json_files:
        print(f"âŒ {input_dir} í´ë”ì— JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… {len(json_files)}ê°œ JSON íŒŒì¼ ë°œê²¬")
    print(f"ğŸ“‚ ì…ë ¥ í´ë”: {input_dir}")
    print(f"ğŸ“‚ ì¶œë ¥ í´ë”: json_merge/")
    
    # íŒŒì¼ëª… íŒ¨í„´ ì•ˆë‚´
    print(f"\nğŸ“ ì§€ì›í•˜ëŠ” íŒŒì¼ëª… íŒ¨í„´:")
    print(f"   â€¢ ë¶„ë¥˜: ë¶„ë¥˜_ì„¸ì…˜ID_ë²ˆí˜¸.json")
    print(f"   â€¢ ìš”ì•½: ìš”ì•½_ì„¸ì…˜ID_ë²ˆí˜¸.json")
    print(f"   â€¢ ì§ˆì˜ì‘ë‹µ: ì§ˆì˜ì‘ë‹µ_ì„¸ì…˜ID_ë²ˆí˜¸.json")
    print(f"   (ë˜ëŠ” ì˜ì–´: classification, summary, qa)")
    
    # ì „ì²˜ë¦¬ ì‹¤í–‰
    processor = UnifiedPreprocessor(input_dir)
    processor.run()

if __name__ == "__main__":
    main() 