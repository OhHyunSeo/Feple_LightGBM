# -*- coding: utf-8 -*-
"""2. coloums_extraction_v3_json2csv.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PzBp0ubXM70-70PHK9xDIppfbB8uvqc4
"""

"""## 텍스트 기반 특성 추출 - 샘플 버전"""

# pip install transformers

"""## json_merge/integration_data 기반으로 데이터셋 제작"""

# pip install konlpy
'''
import torch
from transformers import pipeline

# 1) GPU 사용 가능 확인
print("CUDA available:", torch.cuda.is_available())

# 2) device 설정: GPU가 있으면 0, 없으면 -1(CPU 모드)
device = 0 if torch.cuda.is_available() else -1

# 3) sentiment-analysis 파이프라인 생성 (device=0 으로 GPU 사용)
sentiment = pipeline(
    'sentiment-analysis',
    model='nlptown/bert-base-multilingual-uncased-sentiment',
    top_k=None,
    device=device
)

# 확인
print("Pipeline device:", sentiment.device)

import os
import glob
import json
import re
from collections import Counter
from tqdm import tqdm
import pandas as pd
from konlpy.tag import Okt
import torch
from transformers import pipeline

# ——— 세팅 ———
okt = Okt()

# GPU 사용 설정
device = 0 if torch.cuda.is_available() else -1
print("Using device:", "cuda:0" if device == 0 else "cpu")

sentiment = pipeline(
    'sentiment-analysis',
    model='nlptown/bert-base-multilingual-uncased-sentiment',
    top_k=None,
    device=device
)

# 키워드 사전 (예시 — 실제 프로젝트에 맞게 확장하세요)
script_phrases       = ["안녕하세요", "감사합니다", "반갑습니다"]
honorific_endings    = ["습니다", "세요", "니다"]
positive_words       = ["좋다", "만족", "행복", "감사"]
euphonious_words     = ["해주", "드리"]
confirmation_phrases = ["확인", "맞으신", "괜찮을까요"]
empathy_phrases      = ["이해합니다", "공감"]
apology_phrases      = ["죄송", "미안"]
request_phrases      = ["부탁", "요청"]
alternative_phrases  = ["하실 수 있습니다", "제안합니다", "추천드립니다"]
conflict_words       = ["아닙니다", "불가", "불편"]
prohibit_words       = ["욕설1", "욕설2"]

def split_sentences(text):
    return re.split(r'(?<=[\\\.!\?])\\s+', text)

def extract_text_features(record, batch_size=32):
    content = record.get('consulting_content', '')

    # 1) ASR 세그먼트 & 발화 수
    segments = []
    for line in content.split('\n'):
        m = re.match(r'^(상담사|고객|손님):\s*(.+)', line)
        if m:
            segments.append({'speaker': m.group(1), 'text': m.group(2)})
    speech_count = len(segments)

    # 2) top_nouns
    all_nouns = []
    for seg in segments:
        all_nouns += okt.nouns(seg['text'])
    top_nouns = [w for w,_ in Counter(all_nouns).most_common(10)]

    # 3) 감정 분석 (배치 처리)
    sentences = [s for s in split_sentences(content) if s.strip()]
    agg = Counter({i:0.0 for i in range(1,6)})
    n_sents = 0
    if sentences:
        batch_scores = sentiment(
            sentences,
            truncation=True,
            batch_size=batch_size
        )
        for scores in batch_scores:
            for d in scores:
                star = int(d['label'][0])
                agg[star] += d['score']
        n_sents = len(sentences)
    emo = {f'emo_{i}_star_score': (agg[i]/n_sents if n_sents else 0.0) for i in range(1,6)}
    sent_score = sum(i * emo[f'emo_{i}_star_score'] for i in range(1,6))
    if   sent_score >= 3.5: sent_label = "긍정"
    elif sent_score >= 2.5: sent_label = "중립"
    else:                   sent_label = "부정"

    # 4) 분류: 상담 주제 / 상담 내용
    mid_category = None
    content_category = None
    for inst in record.get('instructions', []):
        items = inst.get('data', [inst])
        for d in items:
            if d.get('task_category') == '상담 주제':
                mid_category = d.get('output')
            elif d.get('task_category') == '상담 내용':
                content_category = d.get('output')

    # 5) 대화 장소
    pm = re.search(r'(\w+(센터|매장|지점))', content)
    rec_place = pm.group(1) if pm else None

    # 6) 비율/카운트 헬퍼
    def ratio(keys):
        tot = len(content.split())
        return sum(content.count(k) for k in keys) / tot if tot else 0
    def count(keys):
        return sum(content.count(k) for k in keys)

    # 7) 기타 지표 결합
    features = {
        'session_id':                   record.get('session_id'),
        'speech_count':                 speech_count,
        'top_nouns':                    ','.join(top_nouns),
        **emo,
        'sent_score':                   sent_score,
        'sent_label':                   sent_label,
        'mid_category':                 mid_category,
        'content_category':             content_category,
        'rec_place':                    rec_place,
        'script_phrase_ratio':          ratio(script_phrases),
        'honorific_ratio':              ratio(honorific_endings),
        'positive_word_ratio':          ratio(positive_words),
        'euphonious_word_ratio':        ratio(euphonious_words),
        'confirmation_ratio':           ratio(confirmation_phrases),
        'empathy_ratio':                ratio(empathy_phrases),
        'apology_ratio':                ratio(apology_phrases),
        'request_ratio':                ratio(request_phrases),
        'alternative_suggestion_count': count(alternative_phrases),
        'conflict_flag':                int(any(w in content for w in conflict_words)),
        'manual_compliance_ratio':      1 - (count(prohibit_words) / max(1, speech_count))
    }
    return features

# ——— 모든 세션 JSON 파일 순회 & 처리 ———
all_files = glob.glob(
    '/content/drive/MyDrive/Gwangjin_gu/preprocessing_call/json_merge_all/integration_data_v3_with_meta/**/*.json',
    recursive=True
)
rows = []
for fp in tqdm(all_files, desc='Processing all sessions'):
    with open(fp, 'r', encoding='utf-8') as f:
        rec = json.load(f)
    rows.append(extract_text_features(rec))

# ——— DataFrame 생성 및 CSV 저장 ———
df = pd.DataFrame(rows)
output_dir = '/content/drive/MyDrive/Gwangjin_gu/preprocessing_call/colab/output_columns_all'
os.makedirs(output_dir, exist_ok=True)
out_csv = os.path.join(output_dir, 'text_features_all_colab_ver.csv')
df.to_csv(out_csv, index=False, encoding='utf-8-sig')

print(f'모든 세션 처리 완료, CSV → {out_csv}')

"""#### 데이터셋 제작 새 코드 (샘플로 5개 시행)"""

# -*- coding: utf-8 -*-
import os, glob, json, re
from collections import Counter, defaultdict
from tqdm.auto import tqdm
import pandas as pd
from konlpy.tag import Okt
from transformers import pipeline

# ——— 설정 ———
okt = Okt()
sentiment = pipeline(
    'sentiment-analysis',
    model='nlptown/bert-base-multilingual-uncased-sentiment',
    top_k=None
)

# 키워드 사전 (예시)
script_phrases       = ["안녕하세요", "감사합니다", "반갑습니다"]
honorific_endings    = ["습니다", "세요", "니다"]
positive_words       = ["좋다", "만족", "행복", "감사"]
euphonious_words     = ["해주", "드리"]
confirmation_phrases = ["확인", "맞으신", "괜찮을까요"]
empathy_phrases      = ["이해합니다", "공감"]
apology_phrases      = ["죄송", "미안"]
request_phrases      = ["부탁", "요청"]
alternative_phrases  = ["하실 수 있습니다", "제안합니다", "추천드립니다"]
conflict_words       = ["아닙니다", "불가", "불편"]
prohibit_words       = ["욕설1", "욕설2"]

# ——— 문장 분리 유틸 ———
def split_sentences(text):
    return re.split(r'(?<=[\.!\?])\s+', text)

# ——— 화자별 감정분석 함수 ———
def calc_speaker_emotion(content, speaker_tag):
    lines = [
        line[len(speaker_tag)+1:].strip()
        for line in content.split('\n')
        if line.startswith(f'{speaker_tag}:')
    ]
    sents = []
    for ln in lines:
        for sent in split_sentences(ln):
            if sent.strip():
                sents.append(sent)
    agg = defaultdict(float)
    for sent in tqdm(sents, desc=f'{speaker_tag} 감정분석', leave=False):
        scores = sentiment(sent)[0]
        for d in scores:
            star = int(d['label'][0])
            agg[star] += d['score']
    n = len(sents) or 1
    star_scores = {f'{speaker_tag}_emo_{i}_star_score': agg[i]/n for i in range(1,6)}
    sent_score = sum(i * star_scores[f'{speaker_tag}_emo_{i}_star_score'] for i in range(1,6))
    if   sent_score >= 3.5: label = "긍정"
    elif sent_score >= 2.5: label = "중립"
    else:                  label = "부정"
    star_scores[f'{speaker_tag}_sent_score'] = sent_score
    star_scores[f'{speaker_tag}_sent_label'] = label
    return star_scores

# ——— 세션 특성추출 함수 ———
def extract_text_features(record):
    content = record['consulting_content']
    sid = record['session_id']

    # 1) ASR 세그먼트 & speech_count
    segments = []
    for line in content.split('\n'):
        m = re.match(r'^(상담사|고객|손님):\s*(.+)', line)
        if m:
            segments.append({'speaker': m.group(1), 'text': m.group(2)})
    speech_count = len(segments)

    # 2) 상위 명사
    all_nouns = []
    for seg in segments:
        all_nouns += okt.nouns(seg['text'])
    top_nouns = [w for w,_ in Counter(all_nouns).most_common(10)]

    # 3) 세션 전체 감정 (문장별 평균)
    agg = Counter({i:0.0 for i in range(1,6)})
    n_sents = 0
    for sent in split_sentences(content):
        if not sent.strip(): continue
        scores = sentiment(sent)[0]
        for d in scores:
            star = int(d['label'][0])
            agg[star] += d['score']
        n_sents += 1
    emo = {f'emo_{i}_star_score': (agg[i]/n_sents if n_sents else 0.0) for i in range(1,6)}
    sent_score = sum(i * emo[f'emo_{i}_star_score'] for i in range(1,6))
    if   sent_score >= 3.5: sent_label = "긍정"
    elif sent_score >= 2.5: sent_label = "중립"
    else:                  sent_label = "부정"

    # 4) 지시문(instructions)에서 분류 메타
    mid_cat = None
    cont_cat = None
    for inst in record.get('instructions', []):
        for d in inst.get('data', [inst]):
            if d.get('task_category') == '상담 주제':
                mid_cat = d.get('output')
            elif d.get('task_category') == '상담 내용':
                cont_cat = d.get('output')

    # 5) 비율/카운트 헬퍼
    def ratio(keys):
        tot = len(content.split())
        return sum(content.count(k) for k in keys) / tot if tot else 0
    def count(keys):
        return sum(content.count(k) for k in keys)

    # ——— 기본 피처
    feats = {
        'session_id':                  sid,
        'speech_count':                speech_count,
        'top_nouns':                   ','.join(top_nouns),
        **emo,
        'sent_score':                  sent_score,
        'sent_label':                  sent_label,
        'mid_category':                mid_cat,
        'content_category':            cont_cat,
        'script_phrase_ratio':         ratio(script_phrases),
        'honorific_ratio':             ratio(honorific_endings),
        'positive_word_ratio':         ratio(positive_words),
        'euphonious_word_ratio':       ratio(euphonious_words),
        'confirmation_ratio':          ratio(confirmation_phrases),
        'empathy_ratio':               ratio(empathy_phrases),
        'apology_ratio':               ratio(apology_phrases),
        'request_ratio':               ratio(request_phrases),
        'alternative_suggestion_count':count(alternative_phrases),
        'conflict_flag':               int(any(w in content for w in conflict_words)),
        'manual_compliance_ratio':     1 - (count(prohibit_words)/max(1, speech_count))
    }

    # 6) 고객/상담사별 감정 추가
    # "고객"과 "손님" 둘 다 찾아서 합치기
    customer_emotion_total = {f'고객_emo_{i}_star_score': 0.0 for i in range(1,6)}
    customer_emotion_total.update({'고객_sent_score': 0.0, '고객_sent_label': '중립'})
    
    # "고객" 찾기
    customer_emotion_1 = calc_speaker_emotion(content, '고객')
    # "손님" 찾기
    customer_emotion_2 = calc_speaker_emotion(content, '손님')
    # "손님" 컬럼명을 "고객"으로 변경
    customer_emotion_2_renamed = {}
    for key, value in customer_emotion_2.items():
        new_key = key.replace('손님_', '고객_')
        customer_emotion_2_renamed[new_key] = value
    
    # 둘 중에 실제 데이터가 있는 것을 사용
    if sum(customer_emotion_1[f'고객_emo_{i}_star_score'] for i in range(1,6)) > 0:
        feats.update(customer_emotion_1)
    elif sum(customer_emotion_2_renamed[f'고객_emo_{i}_star_score'] for i in range(1,6)) > 0:
        feats.update(customer_emotion_2_renamed)
    else:
        feats.update(customer_emotion_total)  # 둘 다 없으면 기본값
    
    feats.update(calc_speaker_emotion(content, '상담사'))

    return feats

# ——— 실행 예시: 샘플 5개 추출 및 저장 ———
files = glob.glob('/content/drive/MyDrive/Gwangjin_gu/preprocessing_call/json_merge_all/integration_data_v3/final_merged_*.json')[:5]
rows = []
for fp in tqdm(files, desc='샘플 5개 처리'):
    rec = json.load(open(fp, 'r', encoding='utf-8'))
    rec['consulting_content'] = rec.get('consulting_content') or rec.get('classification',{}).get('consulting_content','')
    rec['instructions'] = rec.get('instructions', [])
    rows.append(extract_text_features(rec))

df = pd.DataFrame(rows)
os.makedirs('/content/drive/MyDrive/Gwangjin_gu/preprocessing_call/colab', exist_ok=True)
df.to_csv('/content/drive/MyDrive/Gwangjin_gu/preprocessing_call/colab/text_features_sample.csv', index=False, encoding='utf-8-sig')
print("샘플 특성추출 완료 → /content/drive/MyDrive/Gwangjin_gu/preprocessing_call/colab/text_features_sample.csv")
'''

#------------------------------------------------- 실제 코드 -------------------------------------------------------------------------
# -*- coding: utf-8 -*-
import os
import glob
import json
import re
from collections import Counter
from tqdm.auto import tqdm
import pandas as pd
from konlpy.tag import Okt
import torch
from transformers import pipeline

# ——— 1) 설정 ———
okt = Okt()
device = 0 if torch.cuda.is_available() else -1
print("Using device:", "cuda:0" if device == 0 else "cpu")

sentiment = pipeline(
    'sentiment-analysis',
    model='nlptown/bert-base-multilingual-uncased-sentiment',
    top_k=None,
    device=device
)

# ——— 키워드 사전 (예시) ———
script_phrases       = ["안녕하세요", "감사합니다", "반갑습니다"]
honorific_endings    = ["습니다", "세요", "니다"]
positive_words       = ["좋다", "만족", "행복", "감사"]
euphonious_words     = ["해주", "드리"]
confirmation_phrases = ["확인", "맞으신", "괜찮을까요"]
empathy_phrases      = ["이해합니다", "공감"]
apology_phrases      = ["죄송", "미안"]
request_phrases      = ["부탁", "요청"]
alternative_phrases  = ["하실 수 있습니다", "제안합니다", "추천드립니다"]
conflict_words       = ["아닙니다", "불가", "불편"]
prohibit_words       = ["욕설1", "욕설2"]

# ——— 2) 유틸리티 함수 ———
def split_sentences(text):
    return re.split(r'(?<=[\\\.!\?])\\s+', text)

def batch_sentiment(sentences, batch_size=32):
    agg = Counter({i:0.0 for i in range(1,6)})
    if not sentences:
        return agg, 0
    for start in range(0, len(sentences), batch_size):
        batch = sentences[start:start+batch_size]
        results = sentiment(batch, truncation=True, batch_size=batch_size)
        for scores in results:
            for d in scores:
                agg[int(d['label'][0])] += d['score']
    return agg, len(sentences)

def calc_speaker_emotion(content, speaker_tag, batch_size=32):
    lines = [
        line.split(':',1)[1].strip()
        for line in content.split('\n')
        if line.startswith(f'{speaker_tag}:')
    ]
    sents = [s for ln in lines for s in split_sentences(ln) if s.strip()]
    agg, n = batch_sentiment(sents, batch_size)
    n = n or 1
    scores = {f'{speaker_tag}_emo_{i}_star_score': agg[i]/n for i in range(1,6)}
    sent_score = sum(i * scores[f'{speaker_tag}_emo_{i}_star_score'] for i in range(1,6))
    label = "긍정" if sent_score>=3.5 else "중립" if sent_score>=2.5 else "부정"
    scores[f'{speaker_tag}_sent_score'] = sent_score
    scores[f'{speaker_tag}_sent_label'] = label
    return scores

def extract_text_features(fp):
    with open(fp, 'r', encoding='utf-8') as f:
        rec = json.load(f)

    content = rec.get('consulting_content', '')
    sid     = rec.get('session_id')

    # 1) ASR segments & speech_count
    segments = []
    for line in content.split('\n'):
        m = re.match(r'^(상담사|고객|손님):\s*(.+)', line)
        if m:
            segments.append({'speaker': m.group(1), 'text': m.group(2)})
    speech_count = len(segments)

    # 2) top nouns
    all_nouns = []
    for seg in segments:
        all_nouns += okt.nouns(seg['text'])
    top_nouns = ','.join([w for w,_ in Counter(all_nouns).most_common(10)])

    # 3) 전체 감정 (배치)
    sents = [s for s in split_sentences(content) if s.strip()]
    agg, n_sents = batch_sentiment(sents)
    emo = {f'emo_{i}_star_score': agg[i]/(n_sents or 1) for i in range(1,6)}
    sent_score = sum(i * emo[f'emo_{i}_star_score'] for i in range(1,6))
    sent_label = "긍정" if sent_score>=3.5 else "중립" if sent_score>=2.5 else "부정"

    # 4) instructions 메타 추출
    mid_cat = None
    cont_cat = None
    for inst in rec.get('instructions', []):
        for d in inst.get('data', [inst]):
            if d.get('task_category') == '상담 주제':
                mid_cat = d.get('output')
            elif d.get('task_category') == '상담 내용':
                cont_cat = d.get('output')

    # 5) 비율/카운트 헬퍼
    def ratio(keys):
        tot = len(content.split())
        return sum(content.count(k) for k in keys)/tot if tot else 0
    def count(keys):
        return sum(content.count(k) for k in keys)

    # 6) 기본 피처 구성
    feats = {
        'session_id':                  sid,
        'speech_count':                speech_count,
        'top_nouns':                   top_nouns,
        **emo,
        'sent_score':                  sent_score,
        'sent_label':                  sent_label,
        'mid_category':                mid_cat,
        'content_category':            cont_cat,
        'script_phrase_ratio':         ratio(script_phrases),
        'honorific_ratio':             ratio(honorific_endings),
        'positive_word_ratio':         ratio(positive_words),
        'euphonious_word_ratio':       ratio(euphonious_words),
        'confirmation_ratio':          ratio(confirmation_phrases),
        'empathy_ratio':               ratio(empathy_phrases),
        'apology_ratio':               ratio(apology_phrases),
        'request_ratio':               ratio(request_phrases),
        'alternative_suggestion_count':count(alternative_phrases),
        'conflict_flag':               int(any(w in content for w in conflict_words)),
        'manual_compliance_ratio':     1 - (count(prohibit_words)/max(1, speech_count))
    }

    # 7) 화자별 감정 추가
    # "고객"과 "손님" 둘 다 찾아서 합치기
    customer_emotion_total = {f'고객_emo_{i}_star_score': 0.0 for i in range(1,6)}
    customer_emotion_total.update({'고객_sent_score': 0.0, '고객_sent_label': '중립'})
    
    # "고객" 찾기
    customer_emotion_1 = calc_speaker_emotion(content, '고객', batch_size=32)
    # "손님" 찾기
    customer_emotion_2 = calc_speaker_emotion(content, '손님', batch_size=32)
    # "손님" 컬럼명을 "고객"으로 변경
    customer_emotion_2_renamed = {}
    for key, value in customer_emotion_2.items():
        new_key = key.replace('손님_', '고객_')
        customer_emotion_2_renamed[new_key] = value
    
    # 둘 중에 실제 데이터가 있는 것을 사용 (더 많은 문장이 있는 쪽)
    if sum(customer_emotion_1[f'고객_emo_{i}_star_score'] for i in range(1,6)) > 0:
        feats.update(customer_emotion_1)
    elif sum(customer_emotion_2_renamed[f'고객_emo_{i}_star_score'] for i in range(1,6)) > 0:
        feats.update(customer_emotion_2_renamed)
    else:
        feats.update(customer_emotion_total)  # 둘 다 없으면 기본값
    
    feats.update(calc_speaker_emotion(content, '상담사'))

    return feats

# ——— 3) 경로 및 체크포인트 설정 ———
files = sorted(glob.glob(
    'json_merge/integration_data/final_merged_*.json'
))
checkpoint_path = 'json_merge/checkpoint.txt'
output_csv      = 'output/text_features_all_v4.csv'

# ——— 4) 재시작 인덱스 로드 ———
if os.path.exists(checkpoint_path):
    start_idx = int(open(checkpoint_path).read().strip())
else:
    start_idx = 0

# ——— 5) 이전에 저장한 결과 불러오기 ———
rows = []
if os.path.exists(output_csv):
    df_prev = pd.read_csv(output_csv, encoding='utf-8-sig')
    rows = df_prev.to_dict(orient='records')

# ——— 6) 남은 파일 순차 처리 & 주기적 체크포인트 저장 ———
for idx in tqdm(range(start_idx, len(files)), desc='전체 세션 처리'):
    fp = files[idx]
    rows.append(extract_text_features(fp))

    # 50개마다 또는 마지막에 중간 저장
    if (idx + 1) % 50 == 0 or idx == len(files) - 1:
        df = pd.DataFrame(rows)
        df.to_csv(output_csv, index=False, encoding='utf-8-sig')
        with open(checkpoint_path, 'w') as f:
            f.write(str(idx + 1))

print("전체 특성추출 완료 →", output_csv)