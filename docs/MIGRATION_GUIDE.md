# ì½”ë“œ ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ

## ğŸ“‹ í˜„ì¬ ë¬¸ì œì  ë° í•´ê²° ë°©ì•ˆ

### ğŸš¨ ê¸°ì¡´ ì½”ë“œì˜ ì£¼ìš” ë¬¸ì œì 

#### 1. **Google Colab íŠ¹í™” ì½”ë“œ**
```python
# âŒ ì œê±°í•´ì•¼ í•  ì½”ë“œë“¤
from google.colab import drive
drive.mount('/content/drive')

!pip install lightgbm
!pip install transformers torch
```

**í•´ê²°ë°©ì•ˆ**: 
- `requirements.txt`ë¡œ ì˜ì¡´ì„± ê´€ë¦¬
- í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ì„¤ì •ìœ¼ë¡œ ë³€ê²½

#### 2. **í•˜ë“œì½”ë”©ëœ ê²½ë¡œ**
```python
# âŒ ë¬¸ì œê°€ ìˆëŠ” ì½”ë“œ
INPUT_DIR  = 'data_v3/classification'
OUTPUT_DIR = 'json_merge_all/classification_merge_output_v3'
CLASS_DIR = '/content/drive/MyDrive/Gwangjin_gu/preprocessing_call/json_merge_all/classification_merge_output_v3'
```

**í•´ê²°ë°©ì•ˆ**:
```python
# âœ… ìˆ˜ì •ëœ ì½”ë“œ
from utils.config import get_settings
settings = get_settings()

INPUT_DIR = settings.data_input_dir
OUTPUT_DIR = settings.data_output_dir
```

#### 3. **íŒŒì¼ ê¸°ë°˜ ì²˜ë¦¬ â†’ API/DB ê¸°ë°˜ ì²˜ë¦¬**
```python
# âŒ ê¸°ì¡´ ë°©ì‹
with zipfile.ZipFile(zip_path, 'r') as z:
    # ZIP íŒŒì¼ ì²˜ë¦¬...

# âŒ CSV íŒŒì¼ ì €ì¥
df.to_csv('dataset/train.csv', index=False)
```

**í•´ê²°ë°©ì•ˆ**:
```python
# âœ… ìƒˆë¡œìš´ ë°©ì‹ - ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜
async def save_to_database(data: Dict[str, Any], session_id: str):
    async with get_db_session() as db:
        record = ProcessedData(
            session_id=session_id,
            data=json.dumps(data),
            created_at=datetime.now()
        )
        db.add(record)
        await db.commit()
```

## ğŸ”„ íŒŒì¼ë³„ ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒì„¸ ê°€ì´ë“œ

### 1ï¸âƒ£ `1_preprocessing_model_v3.py` â†’ `services/preprocessing_service.py`

#### ê¸°ì¡´ ì½”ë“œ ë¬¸ì œì :
- ZIP íŒŒì¼ ì••ì¶• í•´ì œ ë¡œì§
- íŒŒì¼ ì‹œìŠ¤í…œ ì§ì ‘ ì ‘ê·¼
- ë™ê¸°ì  ì²˜ë¦¬

#### ìˆ˜ì • ì‚¬í•­:

```python
# âŒ ê¸°ì¡´ ì½”ë“œ
def extract_zip_files():
    with zipfile.ZipFile(zip_path, 'r') as z:
        for info in tqdm(entries, desc='Unzipping files'):
            # íŒŒì¼ ì••ì¶• í•´ì œ...

# âœ… ìˆ˜ì •ëœ ì½”ë“œ  
class PreprocessingService:
    async def process_callytics_json(
        self, 
        callytics_json: Dict[str, Any], 
        session_id: str
    ) -> Dict[str, Any]:
        """
        Callytics JSONì„ ì§ì ‘ ì²˜ë¦¬ (ZIP íŒŒì¼ ì—†ìŒ)
        """
        try:
            # JSON ë°ì´í„° ê²€ì¦
            validated_data = self._validate_json_structure(callytics_json)
            
            # ì„¸ì…˜ë³„ ë°ì´í„° ë³‘í•© (ê¸°ì¡´ ë¡œì§ í™œìš©)
            merged_data = self._merge_session_data(validated_data)
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            await self._save_preprocessed_data(session_id, merged_data)
            
            return merged_data
            
        except Exception as e:
            logger.error(f"Preprocessing failed for {session_id}: {str(e)}")
            raise
```

### 2ï¸âƒ£ `2_coloums_extraction_v3_json2csv.py` â†’ `services/feature_extraction_service.py`

#### ìˆ˜ì • ì‚¬í•­:

```python
# âŒ ê¸°ì¡´ ì½”ë“œ - GPU ì„¤ì •ì´ ì „ì—­ì 
device = 0 if torch.cuda.is_available() else -1
sentiment = pipeline('sentiment-analysis', device=device)

# âœ… ìˆ˜ì •ëœ ì½”ë“œ - ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ ë‚´ë¶€ë¡œ ì´ë™
class FeatureExtractionService:
    def __init__(self):
        self.device = 0 if torch.cuda.is_available() else -1
        self.sentiment_pipeline = self._init_sentiment_pipeline()
        self.okt = Okt()
    
    def _init_sentiment_pipeline(self):
        """ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”"""
        return pipeline(
            'sentiment-analysis',
            model='nlptown/bert-base-multilingual-uncased-sentiment',
            device=self.device
        )
    
    async def extract_features(
        self, 
        preprocessed_data: Dict[str, Any], 
        session_id: str
    ) -> Dict[str, Any]:
        """
        ì „ì²˜ë¦¬ëœ ë°ì´í„°ì—ì„œ íŠ¹ì„± ì¶”ì¶œ
        """
        # ê¸°ì¡´ extract_text_features ë¡œì§ì„ ë¹„ë™ê¸°ë¡œ ë³€í™˜
        features = await self._extract_text_features_async(preprocessed_data)
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        await self._save_features(session_id, features)
        
        return features
```

### 3ï¸âƒ£ `3_make_dataset.py` â†’ `services/dataset_service.py`

#### ìˆ˜ì • ì‚¬í•­:

```python
# âŒ ê¸°ì¡´ ì½”ë“œ - CSV íŒŒì¼ ê¸°ë°˜
train.to_csv('dataset/train.csv', index=False)
val.to_csv('dataset/val.csv', index=False) 
test.to_csv('dataset/test.csv', index=False)

# âœ… ìˆ˜ì •ëœ ì½”ë“œ - ë©”ëª¨ë¦¬ ê¸°ë°˜ ì²˜ë¦¬
class DatasetService:
    async def create_dataset(
        self, 
        extracted_features: Dict[str, Any], 
        session_id: str
    ) -> Dict[str, Any]:
        """
        íŠ¹ì„± ë°ì´í„°ì—ì„œ í•™ìŠµìš© ë°ì´í„°ì…‹ ìƒì„±
        """
        # ê¸°ì¡´ ë ˆì´ë¸” ì¶”ì¶œ ë¡œì§ í™œìš©
        labels = self._extract_labels(extracted_features)
        
        # íŠ¹ì„±ê³¼ ë ˆì´ë¸” ë³‘í•©
        dataset = self._merge_features_and_labels(extracted_features, labels)
        
        # ë°ì´í„° ë¶„í•  (ì‹¤ì‹œê°„ ì˜ˆì¸¡ìš©ì´ë¯€ë¡œ ë¶„í•  í•„ìš” ì—†ì„ ìˆ˜ë„)
        processed_dataset = self._prepare_for_prediction(dataset)
        
        return processed_dataset
```

### 4ï¸âƒ£ `4_simple_model_v2.py` â†’ `services/prediction_service.py`

#### ìˆ˜ì • ì‚¬í•­:

```python
# âŒ ê¸°ì¡´ ì½”ë“œ - CSV íŒŒì¼ ë¡œë“œ
train = pd.read_csv('dataset/train.csv')
val = pd.read_csv('dataset/val.csv')
test = pd.read_csv('dataset/test.csv')

# âœ… ìˆ˜ì •ëœ ì½”ë“œ - ì‹¤ì‹œê°„ ì˜ˆì¸¡
class PredictionService:
    def __init__(self):
        self.model = self._load_trained_model()
        self.label_encoder = self._load_label_encoder()
    
    async def predict(
        self, 
        dataset_info: Dict[str, Any], 
        session_id: str
    ) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ì„¸ì…˜ì— ëŒ€í•œ ì‹¤ì‹œê°„ ì˜ˆì¸¡
        """
        try:
            # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
            X = self._prepare_features(dataset_info)
            
            # ì˜ˆì¸¡ ì‹¤í–‰
            prediction_proba = self.model.predict_proba(X)
            prediction = self.model.predict(X)[0]
            confidence = max(prediction_proba[0])
            
            # ê²°ê³¼ í›„ì²˜ë¦¬
            prediction_label = self.label_encoder.inverse_transform([prediction])[0]
            
            result = {
                "prediction": prediction_label,
                "confidence": float(confidence),
                "session_id": session_id,
                "model_version": "v2.0",
                "features": dataset_info
            }
            
            return result
            
        except Exception as e:
            logger.error(f"Prediction failed for {session_id}: {str(e)}")
            raise
```

## ğŸ”§ í™˜ê²½ ì„¤ì • ìˆ˜ì •ì‚¬í•­

### `utils/config.py` ìƒì„±
```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
    database_url: str
    
    # ëª¨ë¸ ì„¤ì •
    model_path: str = "/app/models/"
    gpu_enabled: bool = True
    batch_size: int = 32
    
    # API ì„¤ì •
    api_workers: int = 4
    api_port: int = 8000
    
    # Callytics ì—°ë™ ì„¤ì •
    callytics_api_endpoint: str
    callytics_api_key: str
    
    class Config:
        env_file = ".env"

def get_settings():
    return Settings()
```

### `requirements.txt` ì‘ì„±
```txt
# ê¸°ë³¸ ML ë¼ì´ë¸ŒëŸ¬ë¦¬
lightgbm==4.1.0
transformers==4.35.0
torch==2.1.0
konlpy==0.6.0
pandas==2.1.0
numpy==1.24.0
scikit-learn==1.3.0
tqdm==4.66.0

# API í”„ë ˆì„ì›Œí¬
fastapi==0.104.0
uvicorn[standard]==0.24.0
pydantic==2.5.0
pydantic-settings==2.1.0

# ë°ì´í„°ë² ì´ìŠ¤
sqlalchemy==2.0.0
asyncpg==0.29.0  # PostgreSQL
alembic==1.12.0  # DB ë§ˆì´ê·¸ë ˆì´ì…˜

# ìºì‹± ë° ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…
redis==5.0.0
celery==5.3.0

# ê¸°íƒ€
python-multipart==0.0.6
python-dotenv==1.0.0
```

## ğŸš€ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰ ìˆœì„œ

### 1. í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„±
```bash
mkdir -p {api,services,database,utils,core}/{routers,models}
```

### 2. ê¸°ì¡´ íŒŒì¼ë“¤ì„ `core/` ë””ë ‰í† ë¦¬ë¡œ ì´ë™
```bash
mv *.py core/
```

### 3. ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ë“¤ ìƒì„±
- `services/preprocessing_service.py`
- `services/feature_extraction_service.py`
- `services/dataset_service.py`
- `services/prediction_service.py`

### 4. API ì—”ë“œí¬ì¸íŠ¸ ìƒì„±
- `api/routers/callytics_integration.py`
- `api/models/request_models.py`
- `api/models/response_models.py`

### 5. ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
- `database/models.py`
- `database/connection.py`

### 6. ì„¤ì • ë° ìœ í‹¸ë¦¬í‹°
- `utils/config.py`
- `utils/auth.py`
- `utils/helpers.py`

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **GPU ë©”ëª¨ë¦¬ ê´€ë¦¬**: ì—¬ëŸ¬ ì„¸ì…˜ ë™ì‹œ ì²˜ë¦¬ ì‹œ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ê°€ëŠ¥
2. **ë¹„ë™ê¸° ì²˜ë¦¬**: ê°ì • ë¶„ì„ ê°™ì€ ë¬´ê±°ìš´ ì‘ì—…ì€ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬
3. **ì—ëŸ¬ í•¸ë“¤ë§**: ê° ë‹¨ê³„ë³„ ì‹¤íŒ¨ ì‹œ ë¡¤ë°± ë¡œì§ í•„ìš”
4. **ìºì‹±**: ë°˜ë³µì ì¸ ëª¨ë¸ ë¡œë”©ì„ í”¼í•˜ê¸° ìœ„í•œ ìºì‹± ì „ëµ
5. **ë¡œê¹…**: ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸í•œ ë¡œê¹… ì‹œìŠ¤í…œ

ì´ë ‡ê²Œ ë§ˆì´ê·¸ë ˆì´ì…˜í•˜ë©´ Callytics â†’ LightGBMì˜ ë§¤ë„ëŸ¬ìš´ ë°ì´í„° í”Œë¡œìš°ê°€ êµ¬ì¶•ë©ë‹ˆë‹¤. 