# -*- coding: utf-8 -*-
"""3. make_dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cGLKa6PkH4PivrYv4fafSc9hkhBl0pJF
"""

from google.colab import drive
drive.mount('/content/drive')

"""## 1. 예측할 레이블 (평가 지표) 정의하기

##### 우선 무엇을 예측할지를 정해야 합니다.
예시로, “상담 결과”(해결 불가 / 만족 / 미흡 / 추가 상담 필요)를 레이블로 삼아보겠습니다.

팁: 이 값은 이미 여러분이 만든 merged_classification_*_final.json 안의
instructions 블록 중, task_category == "상담 결과" 의 output 필드에 들어 있습니다.

##### 1-1) 분류 JSON에서 상담 결과만 뽑아서 DataFrame으로 만들기
"""

import glob, json
import pandas as pd
from tqdm import tqdm

# classification 결과 JSON 폴더
CLASS_DIR = '/content/drive/MyDrive/Gwangjin_gu/preprocessing_call/json_merge_all/classification_merge_output_v3'
# 예: output_classification/merged_classification_40001_final.json 등

rows = []
for fp in tqdm(glob.glob(f'{CLASS_DIR}/merged_classification_*_final.json'), desc="Loading labels"):
    rec = json.load(open(fp, 'r', encoding='utf-8'))
    sid = rec['session_id']
    # 분류 리스트에서 첫 아이템만 보자 (세션당 동일 상담 내용)
    block = rec['분류'][0]
    # 그 block 안의 instructions[0]['data'] 에서 '상담 결과'를 찾기
    label = None
    for d in block['instructions'][0]['data']:
        if d['task_category'] == '상담 결과':
            label = d['output']
            break
    rows.append({'session_id': sid, 'result_label': label})

df_labels = pd.DataFrame(rows)
print(df_labels['result_label'].value_counts())

df_labels.tail()

import os

# 1번에서 생성된 df_labels DataFrame 을 CSV로 저장하기
os.makedirs('/content/drive/MyDrive/Gwangjin_gu/preprocessing_call/columns_extraction_all/preprocessing', exist_ok=True)
df_labels.to_csv('/content/drive/MyDrive/Gwangjin_gu/preprocessing_call/columns_extraction_all/preprocessing/session_labels.csv', index=False, encoding='utf-8-sig')

print("✅ 세션별 레이블 CSV 저장 완료 → columns_extraction_all/preprocessing/session_labels.csv")

"""## 2. 특성 csv와 레이블 합치기 (JOIN)

이제 1단계에서 만든 레이블(df_labels)을, 이전에 뽑아둔 텍스트·메타 특성 CSV(text_features_all.csv)와 session_id 기준으로 합치겠습니다.
"""

import pandas as pd

# 1) 불러오기
df_feat   = pd.read_csv('/content/drive/MyDrive/Gwangjin_gu/preprocessing_call/column_extraction/text_features_all_v4_with_text_and_segments.csv', encoding='utf-8-sig')
df_labels = pd.read_csv('/content/drive/MyDrive/Gwangjin_gu/preprocessing_call/columns_extraction_all/preprocessing/session_labels.csv', encoding='utf-8-sig')  # 혹은 위에서 만든 df_labels.to_csv(...) 파일

# 2) session_id 자료형 통일 (둘 다 str으로)
df_feat['session_id']   = df_feat['session_id'].astype(str)
df_labels['session_id'] = df_labels['session_id'].astype(str)

# 3) 병합
df = df_feat.merge(df_labels, on='session_id', how='inner')

# 4) 확인
print(f"결측 레이블: {df['result_label'].isna().sum()}개")
print(f"총 샘플: {len(df)}개, 레이블 분포:\n", df['result_label'].value_counts())

df.head()

"""## 3. 학습/검증/시험용 데이터 분할

##### result_label 의 분포 비율(클래스 불균형)을 유지하며 Stratified Split 합니다.
"""

from sklearn.model_selection import train_test_split

# 3-1) Test 세트 분리 (15%)
trainval, test = train_test_split(
    df,
    test_size=0.15,
    stratify=df['result_label'],
    random_state=42
)

# 3-2) Train / Validation 분리 (전체 중 15%가 Validation → trainval 의 17.6% 약 15%)
train, val = train_test_split(
    trainval,
    test_size=0.1765,
    stratify=trainval['result_label'],
    random_state=42
)

print("train:", train.shape, "val:", val.shape, "test:", test.shape)

"""## 4. 피처 선택 및 전처리
1. 불필요 컬럼 제거
- asr_segments 같은 리스트/텍스트 컬럼은 모델 입력용으로 가공하거나 제거
- top_nouns 는 나중에 TF-IDF → embedding 용으로 따로 저장 가능

2. 인코딩
- 범주형 컬럼(mid_category, content_category, rec_place, result_label) → LabelEncoder or OneHot

3. 스케일링
- 수치형 (speech_count, 비율 컬럼 등) → StandardScaler 또는 MinMaxScaler
"""

# 예시: 범주형 인코딩
from sklearn.preprocessing import LabelEncoder

le_topic = LabelEncoder().fit(df['mid_category'])
train['mid_category_id'] = le_topic.transform(train['mid_category'])
val['mid_category_id']   = le_topic.transform(val['mid_category'])
test['mid_category_id']  = le_topic.transform(test['mid_category'])

# 결과 레이블도 숫자로
le_res = LabelEncoder().fit(df['result_label'])
train['label_id'] = le_res.transform(train['result_label'])
val['label_id']   = le_res.transform(val['result_label'])
test['label_id']  = le_res.transform(test['result_label'])

"""## 5. 최종 데이터셋 csv로 저장"""

os.makedirs('/content/drive/MyDrive/Gwangjin_gu/preprocessing_call/dataset_v4', exist_ok=True)
train.to_csv('/content/drive/MyDrive/Gwangjin_gu/preprocessing_call/dataset_v4/train.csv', index=False, encoding='utf-8-sig')
val.to_csv  ('/content/drive/MyDrive/Gwangjin_gu/preprocessing_call/dataset_v4/val.csv',   index=False, encoding='utf-8-sig')
test.to_csv ('/content/drive/MyDrive/Gwangjin_gu/preprocessing_call/dataset_v4/test.csv',  index=False, encoding='utf-8-sig')
print("✅ Dataset saved under dataset_v4/")