# -*- coding: utf-8 -*-
"""
ìƒë‹´ ë°ì´í„° ìë™ ì²˜ë¦¬ ì‹œìŠ¤í…œ
- JSON íŒŒì¼ ìë™ ê°ì§€ ë° ì²˜ë¦¬
- íŒŒì´í”„ë¼ì¸ ìë™ ì‹¤í–‰
"""

import os
import sys
import time
import threading
import subprocess
import shutil
from pathlib import Path
from datetime import datetime
import pandas as pd

# watchdogì´ ì—†ìœ¼ë©´ ì„¤ì¹˜ ì•ˆë‚´
try:
    from watchdog.observers import Observer
    from watchdog.events import FileSystemEventHandler
except ImportError:
    print("âŒ watchdog ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    print("ì„¤ì¹˜ ëª…ë ¹: pip install watchdog")
    sys.exit(1)

class FileMonitorHandler(FileSystemEventHandler):
    """íŒŒì¼ ë³€ê²½ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬"""
    
    def __init__(self, processor):
        self.processor = processor
        self.last_processed = {}  # íŒŒì¼ë³„ ë§ˆì§€ë§‰ ì²˜ë¦¬ ì‹œê°„
        self.processing_lock = threading.Lock()
        
    def on_created(self, event):
        """ìƒˆ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆì„ ë•Œ"""
        if not event.is_directory:
            self._handle_file_event(event.src_path, "ìƒì„±ë¨")
    
    def on_modified(self, event):
        """íŒŒì¼ì´ ìˆ˜ì •ë˜ì—ˆì„ ë•Œ"""
        if not event.is_directory:
            self._handle_file_event(event.src_path, "ìˆ˜ì •ë¨")
    
    def _handle_file_event(self, file_path, event_type):
        """íŒŒì¼ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        # JSON íŒŒì¼ë§Œ ì²˜ë¦¬
        if not file_path.endswith('.json'):
            return
        
        # data/input í´ë”ì˜ íŒŒì¼ë§Œ ì²˜ë¦¬
        path_obj = Path(file_path)
        if 'data' not in path_obj.parts or 'input' not in path_obj.parts:
            return
        
        # ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€ (5ì´ˆ ê°„ê²©)
        current_time = time.time()
        if (file_path in self.last_processed and 
            current_time - self.last_processed[file_path] < 5):
            return
        
        self.last_processed[file_path] = current_time
        
        # íŒŒì¼ì´ ì™„ì „íˆ ì“°ì—¬ì§ˆ ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°
        time.sleep(2)
        
        # ì²˜ë¦¬ ì‹¤í–‰ (ìŠ¤ë ˆë“œ ì•ˆì „)
        with self.processing_lock:
            print(f"\nğŸ”” íŒŒì¼ {event_type}: {file_path}")
            self.processor.process_new_file(file_path)

class AutoProcessor:
    """ìë™ ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.is_processing = False
        self.processed_files = set()
        
        # ì²˜ë¦¬ ê¸°ë¡ íŒŒì¼ ë¡œë“œ
        self.log_file = Path("auto_processing_log.txt")
        self._load_processed_files()
        
        # íŒŒì¼ ë¶„ë¥˜ ë§¤í•‘
        self.file_classification_map = {
            'ë¶„ë¥˜': 'classification',
            'ìš”ì•½': 'summary', 
            'ì§ˆì˜ì‘ë‹µ': 'qa'
        }
        
        # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
        self._ensure_directories()
    
    def _ensure_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ë“¤ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ìƒì„±"""
        directories = [
            Path("data/input"),
            Path("data/classification"),
            Path("data/summary"),
            Path("data/qa")
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
        
        print("ğŸ“ í•„ìš”í•œ ë””ë ‰í† ë¦¬ í™•ì¸/ìƒì„± ì™„ë£Œ")
        print(f"   - {Path('data/input').absolute()}")
        print(f"   - {Path('data/classification').absolute()}")
        print(f"   - {Path('data/summary').absolute()}")
        print(f"   - {Path('data/qa').absolute()}")
    
    def _classify_and_move_file(self, file_path):
        """íŒŒì¼ëª…ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ í´ë”ë¡œ ì´ë™"""
        path_obj = Path(file_path)
        filename = path_obj.name
        
        # íŒŒì¼ëª…ì—ì„œ í•œê¸€ í‚¤ì›Œë“œ ì°¾ê¸°
        target_folder = None
        for korean_keyword, folder_name in self.file_classification_map.items():
            if korean_keyword in filename:
                target_folder = folder_name
                break
        
        if target_folder is None:
            print(f"âš ï¸ íŒŒì¼ ë¶„ë¥˜ ì‹¤íŒ¨: '{filename}'ì—ì„œ í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            print(f"   ì§€ì›ë˜ëŠ” í‚¤ì›Œë“œ: {list(self.file_classification_map.keys())}")
            return None
        
        # ëŒ€ìƒ ë””ë ‰í† ë¦¬ ì„¤ì •
        target_dir = Path("data") / target_folder
        target_path = target_dir / filename
        
        try:
            # íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš° ë®ì–´ì“°ê¸°
            if target_path.exists():
                print(f"âš ï¸ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•¨: {target_path}")
                target_path.unlink()
            
            # íŒŒì¼ ì´ë™
            shutil.move(str(path_obj), str(target_path))
            print(f"ğŸ“‚ íŒŒì¼ ë¶„ë¥˜ ì™„ë£Œ: {filename}")
            print(f"   '{korean_keyword}' â†’ {target_folder} í´ë”")
            print(f"   ìœ„ì¹˜: {target_path}")
            
            return str(target_path)
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì´ë™ ì‹¤íŒ¨: {e}")
            return None

    def _load_processed_files(self):
        """ì´ì „ì— ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ë¡œë“œ"""
        if self.log_file.exists():
            try:
                with open(self.log_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            self.processed_files.add(line.strip())
                print(f"ğŸ“‹ ì´ì „ ì²˜ë¦¬ ê¸°ë¡: {len(self.processed_files)}ê°œ íŒŒì¼")
            except Exception as e:
                print(f"âš ï¸ ì²˜ë¦¬ ê¸°ë¡ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _log_processed_file(self, file_path):
        """ì²˜ë¦¬ëœ íŒŒì¼ ê¸°ë¡"""
        try:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(f"{file_path}\n")
            self.processed_files.add(file_path)
        except Exception as e:
            print(f"âš ï¸ ì²˜ë¦¬ ê¸°ë¡ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def process_new_file(self, file_path):
        """ìƒˆ íŒŒì¼ ì²˜ë¦¬"""
        # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ì¸ì§€ í™•ì¸
        if file_path in self.processed_files:
            print(f"â­ï¸ ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼: {Path(file_path).name}")
            return
        
        # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ì§€ í™•ì¸
        if self.is_processing:
            print(f"â³ ë‹¤ë¥¸ íŒŒì¼ ì²˜ë¦¬ ì¤‘... ëŒ€ê¸°: {Path(file_path).name}")
            return
        
        self.is_processing = True
        
        try:
            print(f"\n{'='*60}")
            print(f"ğŸš€ ìë™ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
            print(f"ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"íŒŒì¼: {file_path}")
            print(f"{'='*60}")
            
            # 0ë‹¨ê³„: íŒŒì¼ ë¶„ë¥˜ ë° ì´ë™
            print("\n[0ë‹¨ê³„] íŒŒì¼ ë¶„ë¥˜ ë° ì´ë™ ì¤‘...")
            moved_file_path = self._classify_and_move_file(file_path)
            
            if moved_file_path is None:
                print("âŒ 0ë‹¨ê³„ ì‹¤íŒ¨ - íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨")
                return
            
            # ë¶„ë¥˜ëœ íŒŒì¼ì˜ ì„¸ì…˜ ID ì¶”ì¶œ
            session_id = self._extract_session_id(moved_file_path)
            print(f"ğŸ“‹ ì„¸ì…˜ ID: {session_id}")
            
            # í•´ë‹¹ ì„¸ì…˜ì˜ ë‹¤ë¥¸ íƒ€ì… íŒŒì¼ë“¤ì´ ëª¨ë‘ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ í™•ì¸
            session_complete = self._check_session_completion(session_id)
            
            if session_complete:
                print(f"âœ… ì„¸ì…˜ {session_id}ì˜ ëª¨ë“  ë°ì´í„° íƒ€ì…ì´ ì¤€ë¹„ì™„ë£Œ!")
                
                # 1ë‹¨ê³„: JSON ë³‘í•© (ë¶„ë¥˜, ìš”ì•½, ì§ˆì˜ì‘ë‹µ ê°ê° ì²˜ë¦¬)
                print("\n[1ë‹¨ê³„] JSON íŒŒì¼ ë³‘í•© ì¤‘...")
                success = self._run_step("scripts/1_preprocessing_unified.py", "JSON ë³‘í•©")
                
                if not success:
                    print("âŒ 1ë‹¨ê³„ ì‹¤íŒ¨ - íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨")
                    return
                
                # 2ë‹¨ê³„: íŠ¹ì„± ì¶”ì¶œ + ì˜ˆì¸¡ (í†µí•©ëœ íŒŒì¼ ì‚¬ìš©)
                print("\n[2ë‹¨ê³„] íŠ¹ì„± ì¶”ì¶œ + ì˜ˆì¸¡ ì¤‘...")
                success = self._run_step("scripts/2_extract_and_predict.py", "íŠ¹ì„± ì¶”ì¶œ + ì˜ˆì¸¡")
                
                if not success:
                    print("âŒ 2ë‹¨ê³„ ì‹¤íŒ¨ - íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨")
                    return
                
                # ê²°ê³¼ ëˆ„ì 
                self._accumulate_results()
                
                print(f"\n{'='*60}")
                print(f"ğŸ‰ ì„¸ì…˜ {session_id} ì™„ì „ ì²˜ë¦¬ ì™„ë£Œ!")
                print(f"ğŸ“„ ê²°ê³¼ íŒŒì¼: output/text_features_all_v4.csv")
                print(f"{'='*60}")
            else:
                print(f"â³ ì„¸ì…˜ {session_id} ëŒ€ê¸° ì¤‘...")
                print("   ë¶„ë¥˜, ìš”ì•½, ì§ˆì˜ì‘ë‹µ íŒŒì¼ì´ ëª¨ë‘ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°í•©ë‹ˆë‹¤.")
                self._show_session_status(session_id)
            
            # ì²˜ë¦¬ ì™„ë£Œ ê¸°ë¡ (ì›ë³¸ íŒŒì¼ ê²½ë¡œë¡œ ê¸°ë¡)
            self._log_processed_file(file_path)
            
        except Exception as e:
            print(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            
        finally:
            self.is_processing = False
    
    def _extract_session_id(self, file_path):
        """íŒŒì¼ ê²½ë¡œì—ì„œ ì„¸ì…˜ ID ì¶”ì¶œ"""
        filename = Path(file_path).name
        parts = filename.split('_')
        
        # íŒŒì¼ëª… íŒ¨í„´: 01_ë¶„ë¥˜_20593_1.json, 02_ìš”ì•½_20594_2.json, 03_ì§ˆì˜ì‘ë‹µ_20595_3.json
        if len(parts) >= 3:
            return parts[2]  # ì„¸ì…˜ ID ë¶€ë¶„
        return 'unknown'
    
    def _check_session_completion(self, session_id):
        """ì„¸ì…˜ì˜ ëª¨ë“  ë°ì´í„° íƒ€ì…ì´ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        data_dirs = {
            'classification': Path("data/classification"),
            'summary': Path("data/summary"),
            'qa': Path("data/qa")
        }
        
        required_types = ['ë¶„ë¥˜', 'ìš”ì•½', 'ì§ˆì˜ì‘ë‹µ']
        found_types = []
        
        for folder_name, folder_path in data_dirs.items():
            if not folder_path.exists():
                continue
                
            # í•´ë‹¹ ì„¸ì…˜ì˜ íŒŒì¼ë“¤ ì°¾ê¸°
            session_files = list(folder_path.glob(f"*{session_id}*.json"))
            if session_files:
                # íŒŒì¼ëª…ì—ì„œ ë°ì´í„° íƒ€ì… í™•ì¸
                for file_path in session_files:
                    filename = file_path.name
                    for korean_type in required_types:
                        if korean_type in filename and korean_type not in found_types:
                            found_types.append(korean_type)
        
        # 3ê°œ íƒ€ì…ì´ ëª¨ë‘ ìˆëŠ”ì§€ í™•ì¸
        return len(found_types) >= 3
    
    def _show_session_status(self, session_id):
        """ì„¸ì…˜ì˜ í˜„ì¬ ìƒíƒœ í‘œì‹œ"""
        data_dirs = {
            'classification': Path("data/classification"),
            'summary': Path("data/summary"), 
            'qa': Path("data/qa")
        }
        
        type_mapping = {
            'ë¶„ë¥˜': 'classification',
            'ìš”ì•½': 'summary',
            'ì§ˆì˜ì‘ë‹µ': 'qa'
        }
        
        print(f"\nğŸ“Š ì„¸ì…˜ {session_id} ìƒíƒœ:")
        for korean_type, folder_name in type_mapping.items():
            folder_path = data_dirs[folder_name]
            if folder_path.exists():
                session_files = list(folder_path.glob(f"*{session_id}*.json"))
                status = "âœ…" if session_files else "âŒ"
                count = len(session_files)
                print(f"   {status} {korean_type}: {count}ê°œ íŒŒì¼")
            else:
                print(f"   âŒ {korean_type}: í´ë” ì—†ìŒ")

    def _run_step(self, script_name, step_name):
        """ê°œë³„ ë‹¨ê³„ ì‹¤í–‰"""
        try:
            # Windows ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
            env = os.environ.copy()
            env['PYTHONIOENCODING'] = 'utf-8'
            env['PYTHONLEGACYWINDOWSSTDIO'] = '1'
            
            if os.name == 'nt':  # Windows
                try:
                    subprocess.run(['chcp', '65001'], shell=True, capture_output=True, check=False)
                except:
                    pass
            
            # ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
            result = subprocess.run(
                [sys.executable, script_name], 
                capture_output=True, 
                text=True, 
                timeout=600,  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
                env=env,
                encoding='utf-8',
                errors='ignore'
            )
            
            if result.returncode == 0:
                print(f"   âœ… {step_name} ì„±ê³µ")
                return True
            else:
                print(f"   âŒ {step_name} ì‹¤íŒ¨ (ì½”ë“œ: {result.returncode})")
                
                # ì‹¤ì œ ì¶œë ¥ íŒŒì¼ í™•ì¸ìœ¼ë¡œ ì„±ê³µ ì—¬ë¶€ ì¬íŒë‹¨
                if step_name == "JSON ë³‘í•©":
                    if Path("json_merge").exists():
                        print("   ğŸ” ì¶œë ¥ í´ë” í™•ì¸ - ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬")
                        return True
                elif step_name == "íŠ¹ì„± ì¶”ì¶œ + ì˜ˆì¸¡":
                    if Path("output/text_features_all_v4.csv").exists():
                        print("   ğŸ” ì¶œë ¥ íŒŒì¼ í™•ì¸ - ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬")
                        return True
                
                return False
                
        except subprocess.TimeoutExpired:
            print(f"   â° {step_name} íƒ€ì„ì•„ì›ƒ")
            return False
        except Exception as e:
            print(f"   âŒ {step_name} ì˜ˆì™¸: {str(e)}")
            return False
    
    def _accumulate_results(self):
        """ê²°ê³¼ë¥¼ ëˆ„ì  CSVì— ì €ì¥"""
        try:
            current_file = Path("output/text_features_all_v4.csv")
            accumulated_file = Path("output/accumulated_results.csv")
            
            if not current_file.exists():
                print("âš ï¸ í˜„ì¬ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŒ")
                return
            
            # í˜„ì¬ ê²°ê³¼ ë¡œë“œ
            df_current = pd.read_csv(current_file, encoding='utf-8-sig')
            
            # ëˆ„ì  íŒŒì¼ì´ ìˆìœ¼ë©´ ë³‘í•©
            if accumulated_file.exists():
                df_accumulated = pd.read_csv(accumulated_file, encoding='utf-8-sig')
                
                # session_id ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°í•˜ë©° ë³‘í•©
                df_combined = pd.concat([df_accumulated, df_current]).drop_duplicates(
                    subset=['session_id'], keep='last'
                )
                
                print(f"ğŸ“Š ëˆ„ì  ê²°ê³¼ ì—…ë°ì´íŠ¸: {len(df_accumulated)} â†’ {len(df_combined)}ê°œ ì„¸ì…˜")
            else:
                df_combined = df_current
                print(f"ğŸ“Š ëˆ„ì  ê²°ê³¼ ì²« ìƒì„±: {len(df_combined)}ê°œ ì„¸ì…˜")
            
            # ëˆ„ì  íŒŒì¼ ì €ì¥
            df_combined.to_csv(accumulated_file, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ ëˆ„ì  ê²°ê³¼ ì €ì¥: {accumulated_file}")
            
        except Exception as e:
            print(f"âŒ ëˆ„ì  ì €ì¥ ì‹¤íŒ¨: {str(e)}")

class FileMonitor:
    """íŒŒì¼ ëª¨ë‹ˆí„°ë§ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, watch_dir="data/input"):
        self.watch_dir = Path(watch_dir)
        self.processor = AutoProcessor()
        self.observer = Observer()
        
        # ëª¨ë‹ˆí„°ë§ ë””ë ‰í† ë¦¬ í™•ì¸
        if not self.watch_dir.exists():
            print(f"âš ï¸ ëª¨ë‹ˆí„°ë§ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {self.watch_dir}")
            print("   data/input í´ë”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            self.watch_dir.mkdir(parents=True, exist_ok=True)
    
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        print(f"ğŸ” íŒŒì¼ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        print(f"   ê°ì‹œ ë””ë ‰í† ë¦¬: {self.watch_dir.absolute()}")
        print(f"   ëŒ€ìƒ íŒŒì¼: data/input/*.json")
        print(f"   ìë™ ë¶„ë¥˜: ë¶„ë¥˜â†’classification, ìš”ì•½â†’summary, ì§ˆì˜ì‘ë‹µâ†’qa")
        print(f"   ì²˜ë¦¬ ë°©ì‹: ì„¸ì…˜ë³„ë¡œ 3ê°œ íƒ€ì…ì´ ëª¨ë‘ ì¤€ë¹„ë˜ë©´ í†µí•© ì²˜ë¦¬")
        print(f"   ì²˜ë¦¬ ê²°ê³¼: output/text_features_all_v4.csv")
        print(f"   ëˆ„ì  ê²°ê³¼: output/accumulated_results.csv")
        print(f"{'='*50}")
        
        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì„¤ì •
        event_handler = FileMonitorHandler(self.processor)
        
        # ì¬ê·€ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§ ì„¤ì •
        self.observer.schedule(event_handler, str(self.watch_dir), recursive=True)
        
        # ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.observer.start()
        print("âœ… ëª¨ë‹ˆí„°ë§ í™œì„±í™”ë¨ (Ctrl+Cë¡œ ì¢…ë£Œ)")
        
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            print("\nğŸ›‘ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€ ì¤‘...")
            self.stop_monitoring()
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.observer.stop()
        self.observer.join()
        print("âœ… ëª¨ë‹ˆí„°ë§ ì¢…ë£Œë¨")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ìƒë‹´ ë°ì´í„° ìë™ ì²˜ë¦¬ ì‹œìŠ¤í…œ")
    print("="*50)
    
    # í•™ìŠµëœ ëª¨ë¸ í™•ì¸
    # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
    current_dir = Path.cwd()
    model_dir = current_dir / "trained_models"
    
    required_files = [
        "counseling_quality_model.pkl",
        "label_encoder.pkl", 
        "feature_names.pkl"
    ]
    
    # ë””ë²„ê¹…ì„ ìœ„í•œ ì •ë³´ ì¶œë ¥
    print(f"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {current_dir}")
    print(f"ëª¨ë¸ ë””ë ‰í† ë¦¬: {model_dir}")
    print(f"ëª¨ë¸ ë””ë ‰í† ë¦¬ ì¡´ì¬ ì—¬ë¶€: {model_dir.exists()}")
    
    missing_files = [f for f in required_files if not (model_dir / f).exists()]
    if missing_files:
        print(f"âŒ í•™ìŠµëœ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_files}")
        print("   ë¨¼ì € train_from_dataset_v4.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•´ì£¼ì„¸ìš”.")
        
        # ì‹¤ì œ íŒŒì¼ ìƒíƒœ í™•ì¸ì„ ìœ„í•œ ì¶”ê°€ ì •ë³´
        print(f"\nğŸ“‚ {model_dir} ë‚´ íŒŒì¼ ëª©ë¡:")
        if model_dir.exists():
            for file in model_dir.iterdir():
                print(f"   - {file.name}")
        else:
            print("   ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    print("âœ… í•™ìŠµëœ ëª¨ë¸ í™•ì¸ ì™„ë£Œ")
    
    # íŒŒì¼ ëª¨ë‹ˆí„°ë§ ì‹œì‘
    monitor = FileMonitor()
    monitor.start_monitoring()

if __name__ == "__main__":
    main() 