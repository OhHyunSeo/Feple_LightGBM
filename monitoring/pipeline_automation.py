# -*- coding: utf-8 -*-
"""
ê¸°ì¡´ 4ê°œ íŒŒì´ì¬ íŒŒì¼ì„ ì´ìš©í•œ ìë™í™” íŒŒì´í”„ë¼ì¸
- data í´ë”ì— JSON íŒŒì¼ì´ ë“¤ì–´ì˜¤ë©´ ìë™ìœ¼ë¡œ 4ë‹¨ê³„ ì²˜ë¦¬
- 1ë‹¨ê³„: 1_preprocessing_model_v3.py (ì „ì²˜ë¦¬)
- 2ë‹¨ê³„: 2_coloums_extraction_v3_json2csv.py (íŠ¹ì„± ì¶”ì¶œ)
- 3ë‹¨ê³„: 3_make_dataset.py (ë°ì´í„°ì…‹ ìƒì„±)
- 4ë‹¨ê³„: 4_simple_model_v2.py (ëª¨ë¸ í‰ê°€)
"""

import os
import sys
import subprocess
import time
import shutil
from pathlib import Path
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pipeline_automation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DataFileHandler(FileSystemEventHandler):
    """data í´ë”ì˜ JSON íŒŒì¼ ë³€ê²½ì„ ê°ì§€í•˜ëŠ” í•¸ë“¤ëŸ¬"""
    
    def __init__(self, pipeline):
        self.pipeline = pipeline
        
    def on_created(self, event):
        if not event.is_directory and event.src_path.endswith('.json'):
            logger.info(f"ğŸ” ìƒˆ JSON íŒŒì¼ ê°ì§€: {event.src_path}")
            # íŒŒì¼ì´ ì™„ì „íˆ ë³µì‚¬ë  ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°
            time.sleep(2)
            self.pipeline.process_new_file(event.src_path)

class PipelineAutomation:
    """ê¸°ì¡´ 4ê°œ íŒŒì´ì¬ íŒŒì¼ì„ ì´ìš©í•œ ìë™í™” íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, data_dir="data", output_dir="pipeline_results"):
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # ê¸°ì¡´ 4ê°œ íŒŒì´ì¬ íŒŒì¼
        self.scripts = [
            "1_preprocessing_model_v3.py",
            "2_coloums_extraction_v3_json2csv.py", 
            "3_make_dataset.py",
            "4_simple_model_v2.py"
        ]
        
        # ì²˜ë¦¬ ë‹¨ê³„ë³„ ì„¤ëª…
        self.script_descriptions = {
            "1_preprocessing_model_v3.py": "ğŸ“ 1ë‹¨ê³„: ì „ì²˜ë¦¬",
            "2_coloums_extraction_v3_json2csv.py": "ğŸ”§ 2ë‹¨ê³„: í…ìŠ¤íŠ¸ íŠ¹ì„± ì¶”ì¶œ",
            "3_make_dataset.py": "ğŸ“Š 3ë‹¨ê³„: ë°ì´í„°ì…‹ ìƒì„±", 
            "4_simple_model_v2.py": "ğŸ¤– 4ë‹¨ê³„: ëª¨ë¸ í‰ê°€"
        }
        
        # ê° ë‹¨ê³„ë³„ ê²°ê³¼ íŒŒì¼ í™•ì¸
        self.check_scripts_exist()
    
    def check_scripts_exist(self):
        """í•„ìš”í•œ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ë“¤ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        missing_scripts = []
        for script in self.scripts:
            if not Path(script).exists():
                missing_scripts.append(script)
        
        if missing_scripts:
            logger.error(f"âŒ ë‹¤ìŒ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ë“¤ì´ ì—†ìŠµë‹ˆë‹¤: {missing_scripts}")
            sys.exit(1)
        else:
            logger.info("âœ… ëª¨ë“  í•„ìš”í•œ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤.")
    
    def prepare_input_data(self, json_file_path):
        """ì…ë ¥ ë°ì´í„°ë¥¼ ì ì ˆí•œ ìœ„ì¹˜ë¡œ ë³µì‚¬ ë° ì¤€ë¹„"""
        logger.info(f"ğŸ“ ì…ë ¥ ë°ì´í„° ì¤€ë¹„: {json_file_path}")
        
        # json_merge/integration_data ë””ë ‰í† ë¦¬ ìƒì„±
        integration_dir = Path("json_merge/integration_data")
        integration_dir.mkdir(parents=True, exist_ok=True)
        
        # JSON íŒŒì¼ì„ ì ì ˆí•œ ìœ„ì¹˜ë¡œ ë³µì‚¬
        source_file = Path(json_file_path)
        target_file = integration_dir / source_file.name
        
        shutil.copy2(source_file, target_file)
        logger.info(f"âœ… íŒŒì¼ ë³µì‚¬ ì™„ë£Œ: {target_file}")
        
        return target_file
    
    def run_script(self, script_name, step_num):
        """ê°œë³„ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰"""
        logger.info(f"\n{self.script_descriptions[script_name]} ì‹œì‘...")
        
        try:
            # ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
            result = subprocess.run([sys.executable, script_name], 
                                    capture_output=True, 
                                    text=True, 
                                    timeout=300)  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
            
            if result.returncode == 0:
                logger.info(f"âœ… {step_num}ë‹¨ê³„ ì™„ë£Œ: {script_name}")
                if result.stdout:
                    logger.info(f"ì¶œë ¥: {result.stdout[-200:]}")  # ë§ˆì§€ë§‰ 200ìë§Œ ì¶œë ¥
                return True
            else:
                logger.error(f"âŒ {step_num}ë‹¨ê³„ ì‹¤íŒ¨: {script_name}")
                logger.error(f"ì˜¤ë¥˜: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            logger.error(f"â° {step_num}ë‹¨ê³„ íƒ€ì„ì•„ì›ƒ: {script_name}")
            return False
        except Exception as e:
            logger.error(f"ğŸ’¥ {step_num}ë‹¨ê³„ ì˜ˆì™¸ ë°œìƒ: {script_name} - {str(e)}")
            return False
    
    def extract_results(self):
        """ìµœì¢… ê²°ê³¼ ì¶”ì¶œ ë° ì •ë¦¬"""
        logger.info("ğŸ“‹ ìµœì¢… ê²°ê³¼ ì¶”ì¶œ ì¤‘...")
        
        results_summary = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'status': 'completed',
            'output_files': []
        }
        
        # ì£¼ìš” ê²°ê³¼ íŒŒì¼ë“¤ í™•ì¸
        result_files = [
            "output/text_features_all_v4.csv",
            "dataset/train.csv",
            "dataset/val.csv", 
            "dataset/test.csv"
        ]
        
        for file_path in result_files:
            if Path(file_path).exists():
                results_summary['output_files'].append(file_path)
                logger.info(f"âœ… ê²°ê³¼ íŒŒì¼ ìƒì„±ë¨: {file_path}")
        
        # ìµœì¢… í‰ê°€ ê²°ê³¼ í‘œì‹œ
        self.display_final_results()
        
        return results_summary
    
    def display_final_results(self):
        """ìµœì¢… í‰ê°€ ê²°ê³¼ë¥¼ ì½˜ì†”ì— í‘œì‹œ"""
        print("\n" + "="*60)
        print("ğŸ‰ ìƒë‹´ í’ˆì§ˆ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print("="*60)
        
        # CSV íŒŒì¼ì—ì„œ ìµœì‹  ê²°ê³¼ ì½ê¸° (4_simple_model_v2.py ê²°ê³¼)
        try:
            import pandas as pd
            
            # í…ìŠ¤íŠ¸ íŠ¹ì„± íŒŒì¼ í™•ì¸
            feature_file = "output/text_features_all_v4.csv"
            if Path(feature_file).exists():
                df = pd.read_csv(feature_file, encoding='utf-8-sig')
                print(f"ğŸ“Š ì²˜ë¦¬ëœ ì„¸ì…˜ ìˆ˜: {len(df)}")
                print(f"ğŸ“ íŠ¹ì„± ì¶”ì¶œ ê²°ê³¼: {feature_file}")
            
            # ë°ì´í„°ì…‹ íŒŒì¼ë“¤ í™•ì¸
            for dataset_type in ['train', 'val', 'test']:
                dataset_file = f"dataset/{dataset_type}.csv"
                if Path(dataset_file).exists():
                    df_dataset = pd.read_csv(dataset_file, encoding='utf-8-sig')
                    print(f"ğŸ“ˆ {dataset_type.upper()} ë°ì´í„°ì…‹: {len(df_dataset)}í–‰")
            
            print("-"*60)
            print("ğŸ¤– 4ê°œ ì§€í‘œ í‰ê°€ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")
            print("   - ì ê·¹ì„± (Proactivity)")
            print("   - ì¹œí™”ì„± (Friendliness)")  
            print("   - ì „ë¬¸ì„± (Expertise)")
            print("   - ë¬¸ì œí•´ê²°ë ¥ (Problem Solving)")
            
        except Exception as e:
            logger.warning(f"ê²°ê³¼ í‘œì‹œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        print("="*60)
    
    def process_new_file(self, file_path):
        """ìƒˆë¡œìš´ JSON íŒŒì¼ ì²˜ë¦¬"""
        logger.info(f"ğŸš€ íŒŒì´í”„ë¼ì¸ ì‹œì‘: {file_path}")
        
        try:
            # 1. ì…ë ¥ ë°ì´í„° ì¤€ë¹„
            prepared_file = self.prepare_input_data(file_path)
            
            # 2. 4ë‹¨ê³„ ìŠ¤í¬ë¦½íŠ¸ ìˆœì°¨ ì‹¤í–‰
            all_success = True
            for i, script in enumerate(self.scripts, 1):
                success = self.run_script(script, i)
                if not success:
                    logger.error(f"âŒ {i}ë‹¨ê³„ì—ì„œ ì‹¤íŒ¨. íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨.")
                    all_success = False
                    break
                
                # ê° ë‹¨ê³„ ì‚¬ì´ì— ì ì‹œ ëŒ€ê¸°
                time.sleep(1)
            
            # 3. ê²°ê³¼ ì²˜ë¦¬
            if all_success:
                results = self.extract_results()
                logger.info("ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
                return results
            else:
                logger.error("âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨")
                return None
                
        except Exception as e:
            logger.error(f"ğŸ’¥ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def process_batch(self, folder_path):
        """í´ë” ë‚´ ëª¨ë“  JSON íŒŒì¼ ì¼ê´„ ì²˜ë¦¬"""
        json_files = list(Path(folder_path).glob("*.json"))
        
        if not json_files:
            logger.warning(f"ğŸ“ {folder_path}ì— JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        logger.info(f"ğŸ“¦ ì¼ê´„ ì²˜ë¦¬ ì‹œì‘: {len(json_files)}ê°œ íŒŒì¼")
        
        successful = 0
        failed = 0
        
        for json_file in json_files:
            logger.info(f"\n{'='*40}")
            logger.info(f"ì²˜ë¦¬ ì¤‘: {json_file.name}")
            logger.info(f"{'='*40}")
            
            result = self.process_new_file(str(json_file))
            if result:
                successful += 1
            else:
                failed += 1
        
        logger.info(f"\nğŸ“Š ì¼ê´„ ì²˜ë¦¬ ì™„ë£Œ!")
        logger.info(f"   ì„±ê³µ: {successful}ê°œ")
        logger.info(f"   ì‹¤íŒ¨: {failed}ê°œ")
    
    def start_monitoring(self):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        event_handler = DataFileHandler(self)
        observer = Observer()
        
        # data í´ë”ì˜ ëª¨ë“  í•˜ìœ„ í´ë” ëª¨ë‹ˆí„°ë§
        folders_to_monitor = ['classification', 'qa', 'summary']
        
        for folder_name in folders_to_monitor:
            folder_path = self.data_dir / folder_name
            folder_path.mkdir(exist_ok=True)
            observer.schedule(event_handler, str(folder_path), recursive=True)
            logger.info(f"ğŸ‘ï¸ ëª¨ë‹ˆí„°ë§ ì‹œì‘: {folder_path}")
        
        observer.start()
        logger.info("ğŸ” ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ í™œì„±í™”!")
        logger.info("ğŸ“ data í´ë”ì— JSON íŒŒì¼ì„ ë„£ìœ¼ë©´ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.")
        logger.info("â¹ï¸ ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.")
        
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            observer.stop()
            logger.info("âœ‹ ëª¨ë‹ˆí„°ë§ ì¤‘ë‹¨")
        
        observer.join()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    pipeline = PipelineAutomation()
    
    if len(sys.argv) > 1:
        # ë°°ì¹˜ ëª¨ë“œ: íŠ¹ì • í´ë” ì²˜ë¦¬
        folder_path = sys.argv[1]
        pipeline.process_batch(folder_path)
    else:
        # ëŒ€í™”í˜• ëª¨ë“œ
        print("ğŸ¤– ìƒë‹´ í’ˆì§ˆ í‰ê°€ ìë™í™” íŒŒì´í”„ë¼ì¸")
        print("="*50)
        print("1. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        print("2. í´ë” ì¼ê´„ ì²˜ë¦¬")
        print("3. ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬")
        print("4. ì¢…ë£Œ")
        
        while True:
            choice = input("\nì„ íƒ (1-4): ").strip()
            
            if choice == "1":
                pipeline.start_monitoring()
                break
            elif choice == "2":
                folder_path = input("ì²˜ë¦¬í•  í´ë” ê²½ë¡œ: ").strip()
                pipeline.process_batch(folder_path)
            elif choice == "3":
                file_path = input("ì²˜ë¦¬í•  JSON íŒŒì¼ ê²½ë¡œ: ").strip()
                pipeline.process_new_file(file_path)
            elif choice == "4":
                print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            else:
                print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 