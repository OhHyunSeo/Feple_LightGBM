# -*- coding: utf-8 -*-
"""
auto_file_monitor_unified.py
- í†µí•© data í´ë” ëª¨ë‹ˆí„°ë§ ë° ìë™ ì²˜ë¦¬
- íŒŒì¼ëª… ê¸°ë°˜ìœ¼ë¡œ ë¶„ë¥˜/ìš”ì•½/ì§ˆì˜ì‘ë‹µ êµ¬ë¶„
"""

import os
import time
import json
import subprocess
import logging
from datetime import datetime
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
from pathlib import Path
import pandas as pd

class UnifiedFileMonitor(FileSystemEventHandler):
    """í†µí•© íŒŒì¼ ëª¨ë‹ˆí„°ë§ í•¸ë“¤ëŸ¬"""
    
    def __init__(self, monitor_dir="data", output_dir="output"):
        self.monitor_dir = monitor_dir
        self.output_dir = output_dir
        self.processed_files = set()
        self.results_file = os.path.join(output_dir, "accumulated_results.csv")
        
        # ì¶œë ¥ í´ë” ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        # ë¡œê¹… ì„¤ì •
        self.setup_logging()
        
        # ì§€ì›í•˜ëŠ” íŒŒì¼ íƒ€ì…
        self.file_type_keywords = {
            'ë¶„ë¥˜': ['ë¶„ë¥˜', 'classification', 'class'],
            'ìš”ì•½': ['ìš”ì•½', 'summary', 'sum'],
            'ì§ˆì˜ì‘ë‹µ': ['ì§ˆì˜ì‘ë‹µ', 'qa', 'qna', 'question']
        }
        
        self.logger.info("="*60)
        self.logger.info("ğŸ”„ í†µí•© íŒŒì¼ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì‹œì‘")
        self.logger.info(f"ğŸ“ ëª¨ë‹ˆí„°ë§ í´ë”: {monitor_dir}")
        self.logger.info(f"ğŸ“ ì¶œë ¥ í´ë”: {output_dir}")
        self.logger.info("="*60)
    
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_file = os.path.join(self.output_dir, "monitoring.log")
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def detect_file_type(self, filename):
        """íŒŒì¼ëª…ì—ì„œ ë°ì´í„° íƒ€ì… ê°ì§€"""
        filename_lower = filename.lower()
        
        for data_type, keywords in self.file_type_keywords.items():
            for keyword in keywords:
                if keyword in filename_lower:
                    return data_type
        
        return None
    
    def extract_session_id(self, filename):
        """íŒŒì¼ëª…ì—ì„œ ì„¸ì…˜ ID ì¶”ì¶œ"""
        import re
        numbers = re.findall(r'\d+', filename)
        if numbers:
            return numbers[0]  # ì²« ë²ˆì§¸ ìˆ«ìë¥¼ ì„¸ì…˜ IDë¡œ ì‚¬ìš©
        return 'unknown'
    
    def on_created(self, event):
        """ìƒˆ íŒŒì¼ ìƒì„± ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        if event.is_directory:
            return
            
        file_path = event.src_path
        filename = os.path.basename(file_path)
        
        # JSON íŒŒì¼ë§Œ ì²˜ë¦¬
        if not filename.endswith('.json'):
            return
        
        # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ì²´í¬
        if file_path in self.processed_files:
            return
        
        # íŒŒì¼ íƒ€ì… ê°ì§€
        file_type = self.detect_file_type(filename)
        if not file_type:
            self.logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼ íƒ€ì…: {filename}")
            return
        
        session_id = self.extract_session_id(filename)
        
        self.logger.info(f"ğŸ†• ìƒˆ íŒŒì¼ ê°ì§€: {filename}")
        self.logger.info(f"   íƒ€ì…: {file_type}")
        self.logger.info(f"   ì„¸ì…˜ ID: {session_id}")
        
        # íŒŒì¼ì´ ì™„ì „íˆ ìƒì„±ë  ë•Œê¹Œì§€ ëŒ€ê¸°
        self.wait_for_file_complete(file_path)
        
        # ì²˜ë¦¬ ì‹œì‘
        self.process_new_file(file_path, file_type, session_id)
    
    def wait_for_file_complete(self, file_path, timeout=30):
        """íŒŒì¼ì´ ì™„ì „íˆ ìƒì„±ë  ë•Œê¹Œì§€ ëŒ€ê¸°"""
        start_time = time.time()
        last_size = 0
        
        while time.time() - start_time < timeout:
            try:
                current_size = os.path.getsize(file_path)
                if current_size == last_size and current_size > 0:
                    time.sleep(1)  # 1ì´ˆ ë” ëŒ€ê¸°
                    if os.path.getsize(file_path) == current_size:
                        break
                last_size = current_size
                time.sleep(0.5)
            except OSError:
                time.sleep(0.5)
                continue
    
    def process_new_file(self, file_path, file_type, session_id):
        """ìƒˆ íŒŒì¼ ì²˜ë¦¬"""
        try:
            self.logger.info(f"ğŸ”„ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {os.path.basename(file_path)}")
            
            # 1ë‹¨ê³„: ì „ì²˜ë¦¬ (í†µí•© ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©)
            self.logger.info("   [1ë‹¨ê³„] ì „ì²˜ë¦¬ ì‹¤í–‰...")
            preprocess_result = self.run_preprocessing()
            
            if not preprocess_result:
                self.logger.error("âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨")
                return
            
            # 2ë‹¨ê³„: íŠ¹ì§• ì¶”ì¶œ ë° ì˜ˆì¸¡
            self.logger.info("   [2ë‹¨ê³„] íŠ¹ì§• ì¶”ì¶œ ë° ì˜ˆì¸¡...")
            prediction_result = self.run_feature_extraction_and_prediction()
            
            if not prediction_result:
                self.logger.error("âŒ íŠ¹ì§• ì¶”ì¶œ ë° ì˜ˆì¸¡ ì‹¤íŒ¨")
                return
            
            # ê²°ê³¼ ëˆ„ì 
            self.accumulate_results(file_path, file_type, session_id)
            
            # ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ
            self.processed_files.add(file_path)
            
            self.logger.info(f"âœ… íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {os.path.basename(file_path)}")
        
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def run_preprocessing(self):
        """í†µí•© ì „ì²˜ë¦¬ ì‹¤í–‰"""
        try:
            # UTF-8 í™˜ê²½ ì„¤ì •
            env = os.environ.copy()
            env['PYTHONIOENCODING'] = 'utf-8'
            env['PYTHONLEGACYWINDOWSSTDIO'] = '1'
            
            # í†µí•© ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
            cmd = [
                'chcp', '65001', '&&',
                'python', 
                '1_preprocessing_unified.py'
            ]
            
            result = subprocess.run(
                ' '.join(cmd),
                shell=True,
                capture_output=True,
                text=True,
                encoding='utf-8',
                errors='ignore',
                env=env,
                cwd=os.getcwd()
            )
            
            # ì„±ê³µ ì—¬ë¶€ í™•ì¸ (ì¶œë ¥ í´ë” ì¡´ì¬ í™•ì¸)
            success_indicators = [
                os.path.exists('json_merge/classification_merge_output'),
                os.path.exists('json_merge/summary_merge_output'),
                os.path.exists('json_merge/qa_merge_output')
            ]
            
            if any(success_indicators):
                self.logger.info("   âœ… ì „ì²˜ë¦¬ ì™„ë£Œ")
                return True
            else:
                self.logger.error("   âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨ - ì¶œë ¥ í´ë” ì—†ìŒ")
                if result.stderr:
                    # ì‹¤ì œ ì˜¤ë¥˜ë§Œ ì¶œë ¥ (ì¸ì½”ë”© ì˜¤ë¥˜ëŠ” ë¬´ì‹œ)
                    stderr_lines = result.stderr.split('\n')
                    real_errors = [line for line in stderr_lines 
                                 if line.strip() and 
                                 'UnicodeDecodeError' not in line and
                                 'cp949' not in line]
                    if real_errors:
                        self.logger.error(f"   ì˜¤ë¥˜: {'; '.join(real_errors[:3])}")
                return False
                
        except Exception as e:
            self.logger.error(f"   ì „ì²˜ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return False
    
    def run_feature_extraction_and_prediction(self):
        """íŠ¹ì§• ì¶”ì¶œ ë° ì˜ˆì¸¡ ì‹¤í–‰"""
        try:
            # UTF-8 í™˜ê²½ ì„¤ì •
            env = os.environ.copy()
            env['PYTHONIOENCODING'] = 'utf-8'
            env['PYTHONLEGACYWINDOWSSTDIO'] = '1'
            
            # íŠ¹ì§• ì¶”ì¶œ ë° ì˜ˆì¸¡ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
            cmd = [
                'chcp', '65001', '&&',
                'python', 
                '2_extract_and_predict.py'
            ]
            
            result = subprocess.run(
                ' '.join(cmd),
                shell=True,
                capture_output=True,
                text=True,
                encoding='utf-8',
                errors='ignore',
                env=env,
                cwd=os.getcwd()
            )
            
            # ì„±ê³µ ì—¬ë¶€ í™•ì¸ (ê²°ê³¼ íŒŒì¼ ì¡´ì¬ í™•ì¸)
            result_file = 'output/text_features_all_v4.csv'
            if os.path.exists(result_file):
                self.logger.info("   âœ… íŠ¹ì§• ì¶”ì¶œ ë° ì˜ˆì¸¡ ì™„ë£Œ")
                return True
            else:
                self.logger.error("   âŒ íŠ¹ì§• ì¶”ì¶œ ë° ì˜ˆì¸¡ ì‹¤íŒ¨ - ê²°ê³¼ íŒŒì¼ ì—†ìŒ")
                if result.stderr:
                    stderr_lines = result.stderr.split('\n')
                    real_errors = [line for line in stderr_lines 
                                 if line.strip() and 
                                 'UnicodeDecodeError' not in line and
                                 'cp949' not in line]
                    if real_errors:
                        self.logger.error(f"   ì˜¤ë¥˜: {'; '.join(real_errors[:3])}")
                return False
                
        except Exception as e:
            self.logger.error(f"   íŠ¹ì§• ì¶”ì¶œ ë° ì˜ˆì¸¡ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return False
    
    def accumulate_results(self, file_path, file_type, session_id):
        """ê²°ê³¼ë¥¼ ëˆ„ì  íŒŒì¼ì— ì €ì¥"""
        try:
            # ìµœì‹  ê²°ê³¼ íŒŒì¼ ì½ê¸°
            result_file = 'output/text_features_all_v4.csv'
            if not os.path.exists(result_file):
                self.logger.warning("   ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # ìƒˆë¡œìš´ ê²°ê³¼ ë¡œë“œ
            new_results = pd.read_csv(result_file, encoding='utf-8')
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            new_results['processed_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            new_results['source_file'] = os.path.basename(file_path)
            new_results['data_type'] = file_type
            
            # ëˆ„ì  íŒŒì¼ì— ì¶”ê°€
            if os.path.exists(self.results_file):
                # ê¸°ì¡´ ê²°ê³¼ì™€ í•©ì¹˜ê¸°
                existing_results = pd.read_csv(self.results_file, encoding='utf-8')
                combined_results = pd.concat([existing_results, new_results], ignore_index=True)
            else:
                combined_results = new_results
            
            # ì¤‘ë³µ ì œê±° (session_id ê¸°ì¤€)
            combined_results = combined_results.drop_duplicates(subset=['session_id'], keep='last')
            
            # ì €ì¥
            combined_results.to_csv(self.results_file, index=False, encoding='utf-8')
            
            self.logger.info(f"   ğŸ“ˆ ëˆ„ì  ê²°ê³¼ ì €ì¥: {len(combined_results)}ê°œ ì„¸ì…˜")
            
            # ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½
            if 'predicted_label' in new_results.columns:
                prediction_summary = new_results['predicted_label'].value_counts()
                self.logger.info(f"   ğŸ“Š ìƒˆ ì˜ˆì¸¡ ê²°ê³¼:")
                for label, count in prediction_summary.items():
                    self.logger.info(f"      {label}: {count}ê°œ")
            
        except Exception as e:
            self.logger.error(f"   ê²°ê³¼ ëˆ„ì  ì˜¤ë¥˜: {e}")
    
    def generate_summary_report(self):
        """ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not os.path.exists(self.results_file):
            return
        
        try:
            df = pd.read_csv(self.results_file, encoding='utf-8')
            
            report = f"""
ğŸ“Š í†µí•© ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})
{'='*60}

ğŸ“ˆ ì „ì²´ í†µê³„:
  â€¢ ì´ ì²˜ë¦¬ëœ ì„¸ì…˜: {len(df)}ê°œ
  â€¢ ì²˜ë¦¬ëœ ë°ì´í„° íƒ€ì…: {df['data_type'].nunique() if 'data_type' in df.columns else 'N/A'}ê°œ

"""
            
            if 'predicted_label' in df.columns:
                label_counts = df['predicted_label'].value_counts()
                report += "ğŸ¯ ì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬:\n"
                for label, count in label_counts.items():
                    percentage = (count / len(df)) * 100
                    report += f"  â€¢ {label}: {count}ê°œ ({percentage:.1f}%)\n"
                
                if 'confidence' in df.columns:
                    avg_confidence = df['confidence'].mean() * 100  # ë°±ë¶„ìœ¨ë¡œ ë³€í™˜
                    high_confidence = (df['confidence'] >= 0.8).sum()
                    report += f"\nğŸ” ì‹ ë¢°ë„ ë¶„ì„:\n"
                    report += f"  â€¢ í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.1f}%\n"
                    report += f"  â€¢ ê³ ì‹ ë¢°ë„ ì˜ˆì¸¡ (â‰¥80%): {high_confidence}ê°œ ({(high_confidence/len(df)*100):.1f}%)\n"
            
            if 'data_type' in df.columns:
                type_counts = df['data_type'].value_counts()
                report += f"\nğŸ“‹ ë°ì´í„° íƒ€ì…ë³„ ë¶„í¬:\n"
                for data_type, count in type_counts.items():
                    report += f"  â€¢ {data_type}: {count}ê°œ\n"
            
            # ë¦¬í¬íŠ¸ ì €ì¥
            report_file = os.path.join(self.output_dir, "summary_report.txt")
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)
            
            # ë¡œê·¸ì—ë„ ì¶œë ¥
            self.logger.info(report)
            
        except Exception as e:
            self.logger.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
    
    def scan_existing_files(self):
        """ê¸°ì¡´ íŒŒì¼ë“¤ ìŠ¤ìº” ë° ì²˜ë¦¬"""
        self.logger.info("ğŸ” ê¸°ì¡´ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
        
        if not os.path.exists(self.monitor_dir):
            self.logger.warning(f"ëª¨ë‹ˆí„°ë§ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {self.monitor_dir}")
            return
        
        existing_files = []
        for file_path in Path(self.monitor_dir).glob('*.json'):
            filename = file_path.name
            file_type = self.detect_file_type(filename)
            
            if file_type:
                existing_files.append((str(file_path), file_type, self.extract_session_id(filename)))
        
        if existing_files:
            self.logger.info(f"ğŸ“„ {len(existing_files)}ê°œ ê¸°ì¡´ íŒŒì¼ ë°œê²¬")
            
            for file_path, file_type, session_id in existing_files:
                self.logger.info(f"   {file_type} | ì„¸ì…˜ {session_id} | {os.path.basename(file_path)}")
            
            # ì‚¬ìš©ì í™•ì¸
            while True:
                try:
                    response = input(f"\nê¸°ì¡´ íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
                    if response in ['y', 'yes', 'ã…›']:
                        for file_path, file_type, session_id in existing_files:
                            self.process_new_file(file_path, file_type, session_id)
                        break
                    elif response in ['n', 'no', 'ã…œ']:
                        self.logger.info("ê¸°ì¡´ íŒŒì¼ ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                        break
                    else:
                        print("y ë˜ëŠ” nì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                except KeyboardInterrupt:
                    self.logger.info("ì‚¬ìš©ìê°€ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
                    break
        else:
            self.logger.info("ê¸°ì¡´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ í†µí•© í´ë” ê¸°ë°˜ ìë™ ì²˜ë¦¬ ì‹œìŠ¤í…œ")
    print("="*50)
    
    # ëª¨ë‹ˆí„°ë§ ì„¤ì •
    monitor_dir = "data"
    output_dir = "output"
    
    # í´ë” ìƒì„±
    os.makedirs(monitor_dir, exist_ok=True)
    os.makedirs(output_dir, exist_ok=True)
    
    # íŒŒì¼ ëª¨ë‹ˆí„° ìƒì„±
    event_handler = UnifiedFileMonitor(monitor_dir, output_dir)
    
    # ê¸°ì¡´ íŒŒì¼ ìŠ¤ìº”
    event_handler.scan_existing_files()
    
    # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘
    observer = Observer()
    observer.schedule(event_handler, monitor_dir, recursive=False)
    observer.start()
    
    print(f"\nğŸ‘€ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘...")
    print(f"ğŸ“ ëª¨ë‹ˆí„°ë§ í´ë”: {monitor_dir}")
    print(f"ğŸ“ ê²°ê³¼ í´ë”: {output_dir}")
    print(f"\nğŸ“ ì§€ì›í•˜ëŠ” íŒŒì¼ëª… íŒ¨í„´:")
    print(f"   â€¢ ë¶„ë¥˜: ë¶„ë¥˜_ì„¸ì…˜ID_ë²ˆí˜¸.json")
    print(f"   â€¢ ìš”ì•½: ìš”ì•½_ì„¸ì…˜ID_ë²ˆí˜¸.json") 
    print(f"   â€¢ ì§ˆì˜ì‘ë‹µ: ì§ˆì˜ì‘ë‹µ_ì„¸ì…˜ID_ë²ˆí˜¸.json")
    print(f"   (ë˜ëŠ” ì˜ì–´: classification, summary, qa)")
    print(f"\nâš¡ ìƒˆ JSON íŒŒì¼ì„ {monitor_dir} í´ë”ì— ì¶”ê°€í•˜ë©´ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤!")
    print(f"ğŸ›‘ ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.")
    
    try:
        while True:
            time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ìš”ì•½ ë¦¬í¬íŠ¸ ì—…ë°ì´íŠ¸
            event_handler.generate_summary_report()
    
    except KeyboardInterrupt:
        observer.stop()
        print(f"\nğŸ›‘ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ")
        event_handler.generate_summary_report()
    
    observer.join()

if __name__ == "__main__":
    main() 