# -*- coding: utf-8 -*-
"""
pipeline_manager.py
- í†µí•© íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ ì‹œìŠ¤í…œ
- ëª¨ë“  íŒŒì´í”„ë¼ì¸ ì‘ì—…ì„ í†µí•© ê´€ë¦¬
"""

import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime

# í”„ë¡œì íŠ¸ ë‚´ë¶€ ëª¨ë“ˆ
from config import *
from utils import FileUtils, JSONUtils, LoggerUtils, SystemUtils

class PipelineManager:
    """í†µí•© íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, mode: str = "unified"):
        """
        íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        
        Args:
            mode: íŒŒì´í”„ë¼ì¸ ëª¨ë“œ ('unified', 'traditional', 'monitoring')
        """
        self.mode = mode
        self.logger = LoggerUtils.setup_pipeline_logger(f"pipeline_{mode}")
        self.start_time = time.time()
        
        # ì„¤ì • ë¡œê¹…
        LoggerUtils.log_system_info(self.logger)
        LoggerUtils.log_configuration(self.logger, {
            'mode': mode,
            'project_name': PROJECT_NAME,
            'version': VERSION,
            'data_dir': str(DATA_DIR),
            'output_dir': str(OUTPUT_DIR)
        })
        
        # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
        ensure_directories()
        
        # ì‹œìŠ¤í…œ í™˜ê²½ ì„¤ì •
        self.env = SystemUtils.setup_utf8_environment()
        if SystemUtils.is_windows():
            SystemUtils.set_windows_utf8_codepage()
    
    def check_prerequisites(self) -> Tuple[bool, List[str]]:
        """
        íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì „ ì „ì œì¡°ê±´ì„ í™•ì¸í•©ë‹ˆë‹¤.
        
        Returns:
            (ëª¨ë“  ì¡°ê±´ ì¶©ì¡± ì—¬ë¶€, ë¬¸ì œì  ë¦¬ìŠ¤íŠ¸)
        """
        issues = []
        
        # 1. í•„ìˆ˜ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
        required_scripts = [
            PIPELINE_SCRIPTS['preprocessing'],
            PIPELINE_SCRIPTS['preprocessing_unified'],
            PIPELINE_SCRIPTS['feature_extraction'],
            PIPELINE_SCRIPTS['extract_and_predict'],
            PIPELINE_SCRIPTS['prediction']
        ]
        
        for script in required_scripts:
            if not Path(script).exists():
                issues.append(f"í•„ìˆ˜ ìŠ¤í¬ë¦½íŠ¸ ëˆ„ë½: {script}")
        
        # 2. Python íŒ¨í‚¤ì§€ ìš”êµ¬ì‚¬í•­ í™•ì¸
        required_packages = [
            'pandas', 'numpy', 'sklearn', 'lightgbm', 
            'transformers', 'torch', 'konlpy', 'tqdm'
        ]
        
        all_packages_ok, missing_packages = SystemUtils.check_python_requirements(required_packages)
        if not all_packages_ok:
            issues.append(f"ëˆ„ë½ëœ íŒ¨í‚¤ì§€: {', '.join(missing_packages)}")
        
        # 3. í•™ìŠµëœ ëª¨ë¸ íŒŒì¼ í™•ì¸ (ì˜ˆì¸¡ ëª¨ë“œì¸ ê²½ìš°)
        if self.mode in ['unified', 'monitoring']:
            for model_name, model_path in MODEL_FILES.items():
                if not model_path.exists():
                    issues.append(f"ëª¨ë¸ íŒŒì¼ ëˆ„ë½: {model_path}")
        
        # 4. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸
        available_memory = SystemUtils.get_available_memory_gb()
        if available_memory < 2.0:  # ìµœì†Œ 2GB ë©”ëª¨ë¦¬ í•„ìš”
            issues.append(f"ë©”ëª¨ë¦¬ ë¶€ì¡±: {available_memory:.1f}GB (ìµœì†Œ 2GB í•„ìš”)")
        
        success = len(issues) == 0
        
        if success:
            self.logger.info("âœ… ëª¨ë“  ì „ì œì¡°ê±´ì´ ì¶©ì¡±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            self.logger.error("âŒ ì „ì œì¡°ê±´ í™•ì¸ ì‹¤íŒ¨:")
            for issue in issues:
                self.logger.error(f"   - {issue}")
        
        return success, issues
    
    def run_preprocessing(self, unified: bool = True) -> bool:
        """
        ì „ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Args:
            unified: í†µí•© ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            ì‹¤í–‰ ì„±ê³µ ì—¬ë¶€
        """
        self.logger.info("="*60)
        self.logger.info("ğŸ”„ 1ë‹¨ê³„: ì „ì²˜ë¦¬ ì‹œì‘")
        self.logger.info("="*60)
        
        script_name = PIPELINE_SCRIPTS['preprocessing_unified'] if unified else PIPELINE_SCRIPTS['preprocessing']
        
        # ì…ë ¥ ë°ì´í„° í™•ì¸
        if unified:
            json_files = FileUtils.find_files_by_pattern(DATA_DIR, "*.json")
        else:
            # ê¸°ì¡´ ë°©ì‹: í•˜ìœ„ í´ë”ë³„ í™•ì¸
            class_files = FileUtils.find_files_by_pattern(DATA_DIR / "classification", "*.json")
            summary_files = FileUtils.find_files_by_pattern(DATA_DIR / "summary", "*.json")
            qa_files = FileUtils.find_files_by_pattern(DATA_DIR / "qa", "*.json")
            json_files = class_files + summary_files + qa_files
        
        if not json_files:
            self.logger.error("âŒ ì²˜ë¦¬í•  JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        self.logger.info(f"ğŸ“„ ë°œê²¬ëœ JSON íŒŒì¼: {len(json_files)}ê°œ")
        
        # ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        success, stdout, stderr = SystemUtils.run_python_script(
            script_path=script_name,
            timeout=PERFORMANCE['timeout']['preprocessing']
        )
        
        # ê²°ê³¼ í™•ì¸
        if success:
            # ì¶œë ¥ í´ë” í™•ì¸ìœ¼ë¡œ ì„±ê³µ ì—¬ë¶€ íŒë‹¨
            if unified:
                success_indicators = [
                    CLASS_MERGE_DIR.exists(),
                    SUMMARY_MERGE_DIR.exists(),
                    QA_MERGE_DIR.exists()
                ]
            else:
                success_indicators = [CLASS_MERGE_DIR.exists()]
            
            if any(success_indicators):
                self.logger.info("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ")
                return True
            else:
                self.logger.error("âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨ - ì¶œë ¥ í´ë”ê°€ ìƒì„±ë˜ì§€ ì•ŠìŒ")
                return False
        else:
            self.logger.error(f"âŒ ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {stderr}")
            return False
    
    def run_feature_extraction(self) -> bool:
        """
        íŠ¹ì„± ì¶”ì¶œ ë‹¨ê³„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Returns:
            ì‹¤í–‰ ì„±ê³µ ì—¬ë¶€
        """
        self.logger.info("="*60)
        self.logger.info("ğŸ”§ 2ë‹¨ê³„: íŠ¹ì„± ì¶”ì¶œ ì‹œì‘")
        self.logger.info("="*60)
        
        # ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        success, stdout, stderr = SystemUtils.run_python_script(
            script_path=PIPELINE_SCRIPTS['feature_extraction'],
            timeout=PERFORMANCE['timeout']['feature_extraction']
        )
        
        # ê²°ê³¼ í™•ì¸
        if success and RESULT_FILES['features'].exists():
            features_size = FileUtils.get_file_size_mb(RESULT_FILES['features'])
            self.logger.info(f"âœ… íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ - ê²°ê³¼ íŒŒì¼: {features_size:.2f}MB")
            return True
        else:
            self.logger.error(f"âŒ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {stderr}")
            return False
    
    def run_prediction(self) -> bool:
        """
        ì˜ˆì¸¡ ë‹¨ê³„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Returns:
            ì‹¤í–‰ ì„±ê³µ ì—¬ë¶€
        """
        self.logger.info("="*60)
        self.logger.info("ğŸ¤– 4ë‹¨ê³„: ì˜ˆì¸¡ ì‹œì‘")
        self.logger.info("="*60)
        
        # íŠ¹ì„± íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not RESULT_FILES['features'].exists():
            self.logger.error("âŒ íŠ¹ì„± íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € íŠ¹ì„± ì¶”ì¶œì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return False
        
        # ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        success, stdout, stderr = SystemUtils.run_python_script(
            script_path=PIPELINE_SCRIPTS['prediction'],
            timeout=PERFORMANCE['timeout']['prediction']
        )
        
        # ê²°ê³¼ í™•ì¸
        if success and RESULT_FILES['predictions'].exists():
            predictions_size = FileUtils.get_file_size_mb(RESULT_FILES['predictions'])
            self.logger.info(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ - ê²°ê³¼ íŒŒì¼: {predictions_size:.2f}MB")
            return True
        else:
            self.logger.error(f"âŒ ì˜ˆì¸¡ ì‹¤íŒ¨: {stderr}")
            return False
    
    def run_extract_and_predict(self) -> bool:
        """
        íŠ¹ì„± ì¶”ì¶œê³¼ ì˜ˆì¸¡ì„ í†µí•© ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Returns:
            ì‹¤í–‰ ì„±ê³µ ì—¬ë¶€
        """
        self.logger.info("="*60)
        self.logger.info("ğŸ”§ğŸ¤– í†µí•©: íŠ¹ì„± ì¶”ì¶œ + ì˜ˆì¸¡ ì‹œì‘")
        self.logger.info("="*60)
        
        # ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        success, stdout, stderr = SystemUtils.run_python_script(
            script_path=PIPELINE_SCRIPTS['extract_and_predict'],
            timeout=PERFORMANCE['timeout']['feature_extraction'] + PERFORMANCE['timeout']['prediction']
        )
        
        # ê²°ê³¼ í™•ì¸
        if success and RESULT_FILES['features'].exists():
            self.logger.info("âœ… í†µí•© ì²˜ë¦¬ ì™„ë£Œ")
            return True
        else:
            self.logger.error(f"âŒ í†µí•© ì²˜ë¦¬ ì‹¤íŒ¨: {stderr}")
            return False
    
    def run_traditional_pipeline(self) -> bool:
        """
        ê¸°ì¡´ 4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Returns:
            ì‹¤í–‰ ì„±ê³µ ì—¬ë¶€
        """
        self.logger.info("ğŸš€ ê¸°ì¡´ 4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        
        # 1ë‹¨ê³„: ì „ì²˜ë¦¬
        if not self.run_preprocessing(unified=False):
            return False
        
        # 2ë‹¨ê³„: íŠ¹ì„± ì¶”ì¶œ
        if not self.run_feature_extraction():
            return False
        
        # 3ë‹¨ê³„: ë°ì´í„°ì…‹ ìƒì„± (í•„ìš”í•œ ê²½ìš°)
        # í˜„ì¬ëŠ” ì˜ˆì¸¡ ì „ìš©ì´ë¯€ë¡œ ìƒëµ
        
        # 4ë‹¨ê³„: ì˜ˆì¸¡
        if not self.run_prediction():
            return False
        
        return True
    
    def run_unified_pipeline(self) -> bool:
        """
        í†µí•© íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Returns:
            ì‹¤í–‰ ì„±ê³µ ì—¬ë¶€
        """
        self.logger.info("ğŸš€ í†µí•© íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        
        # 1ë‹¨ê³„: í†µí•© ì „ì²˜ë¦¬
        if not self.run_preprocessing(unified=True):
            return False
        
        # 2+4ë‹¨ê³„: íŠ¹ì„± ì¶”ì¶œ + ì˜ˆì¸¡ í†µí•©
        if not self.run_extract_and_predict():
            return False
        
        return True
    
    def generate_report(self) -> Dict[str, Any]:
        """
        íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Returns:
            ì‹¤í–‰ ê²°ê³¼ ë³´ê³ ì„œ
        """
        end_time = time.time()
        execution_time = end_time - self.start_time
        
        report = {
            'pipeline_info': {
                'mode': self.mode,
                'execution_time': execution_time,
                'start_time': datetime.fromtimestamp(self.start_time).isoformat(),
                'end_time': datetime.fromtimestamp(end_time).isoformat()
            },
            'input_data': {},
            'output_data': {},
            'system_info': SystemUtils.get_environment_summary()
        }
        
        # ì…ë ¥ ë°ì´í„° ì •ë³´
        json_files = FileUtils.find_files_by_pattern(DATA_DIR, "*.json")
        report['input_data'] = {
            'total_files': len(json_files),
            'data_size_mb': sum(FileUtils.get_file_size_mb(f) for f in json_files)
        }
        
        # ì¶œë ¥ ë°ì´í„° ì •ë³´
        output_files = {}
        for name, path in RESULT_FILES.items():
            if path.exists():
                output_files[name] = {
                    'path': str(path),
                    'size_mb': FileUtils.get_file_size_mb(path),
                    'exists': True
                }
            else:
                output_files[name] = {
                    'path': str(path),
                    'size_mb': 0,
                    'exists': False
                }
        
        report['output_data'] = output_files
        
        # ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„ (ê°€ëŠ¥í•œ ê²½ìš°)
        if RESULT_FILES['predictions'].exists():
            try:
                import pandas as pd
                df = pd.read_csv(RESULT_FILES['predictions'])
                
                if 'predicted_label' in df.columns:
                    label_counts = df['predicted_label'].value_counts().to_dict()
                    report['prediction_summary'] = {
                        'total_predictions': len(df),
                        'label_distribution': label_counts
                    }
                    
                    if 'confidence' in df.columns:
                        report['prediction_summary']['avg_confidence'] = float(df['confidence'].mean())
                        report['prediction_summary']['high_confidence_count'] = int((df['confidence'] >= 0.8).sum())
            except Exception as e:
                self.logger.warning(f"ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return report
    
    def save_report(self, report: Dict[str, Any]) -> bool:
        """
        ë³´ê³ ì„œë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            report: ë³´ê³ ì„œ ë°ì´í„°
            
        Returns:
            ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        try:
            # JSON ë³´ê³ ì„œ ì €ì¥
            report_file = OUTPUT_DIR / f"pipeline_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            JSONUtils.save_json(report, report_file)
            
            # í…ìŠ¤íŠ¸ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
            summary_lines = [
                f"# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë³´ê³ ì„œ",
                f"",
                f"## ê¸°ë³¸ ì •ë³´",
                f"- ì‹¤í–‰ ëª¨ë“œ: {report['pipeline_info']['mode']}",
                f"- ì‹¤í–‰ ì‹œê°„: {report['pipeline_info']['execution_time']:.2f}ì´ˆ",
                f"- ì‹œì‘ ì‹œê°: {report['pipeline_info']['start_time']}",
                f"- ì¢…ë£Œ ì‹œê°: {report['pipeline_info']['end_time']}",
                f"",
                f"## ì…ë ¥ ë°ì´í„°",
                f"- ì „ì²´ íŒŒì¼ ìˆ˜: {report['input_data']['total_files']}ê°œ",
                f"- ë°ì´í„° í¬ê¸°: {report['input_data']['data_size_mb']:.2f}MB",
                f"",
                f"## ì¶œë ¥ ë°ì´í„°"
            ]
            
            for name, info in report['output_data'].items():
                status = "âœ…" if info['exists'] else "âŒ"
                summary_lines.append(f"- {name}: {status} ({info['size_mb']:.2f}MB)")
            
            if 'prediction_summary' in report:
                ps = report['prediction_summary']
                summary_lines.extend([
                    f"",
                    f"## ì˜ˆì¸¡ ê²°ê³¼",
                    f"- ì´ ì˜ˆì¸¡ ìˆ˜: {ps['total_predictions']}ê°œ",
                    f"- í‰ê·  ì‹ ë¢°ë„: {ps['avg_confidence']:.3f}",
                    f"- ê³ ì‹ ë¢°ë„ ì˜ˆì¸¡: {ps['high_confidence_count']}ê°œ",
                    f"",
                    f"### ë ˆì´ë¸” ë¶„í¬"
                ])
                
                for label, count in ps['label_distribution'].items():
                    percentage = (count / ps['total_predictions']) * 100
                    summary_lines.append(f"- {label}: {count}ê°œ ({percentage:.1f}%)")
            
            # í…ìŠ¤íŠ¸ ë³´ê³ ì„œ ì €ì¥
            summary_file = OUTPUT_DIR / f"pipeline_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(summary_lines))
            
            self.logger.info(f"ğŸ“‹ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ:")
            self.logger.info(f"   - JSON: {report_file}")
            self.logger.info(f"   - ìš”ì•½: {summary_file}")
            
            return True
            
        except Exception as e:
            LoggerUtils.log_error_with_traceback(self.logger, e, "ë³´ê³ ì„œ ì €ì¥")
            return False
    
    def run(self) -> bool:
        """
        íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Returns:
            ì‹¤í–‰ ì„±ê³µ ì—¬ë¶€
        """
        self.logger.info(f"ğŸš€ {PROJECT_NAME} v{VERSION} íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        self.logger.info(f"ğŸ“‹ ì‹¤í–‰ ëª¨ë“œ: {self.mode}")
        
        # ì „ì œì¡°ê±´ í™•ì¸
        prerequisites_ok, issues = self.check_prerequisites()
        if not prerequisites_ok:
            return False
        
        # ëª¨ë“œë³„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        success = False
        
        try:
            if self.mode == "unified":
                success = self.run_unified_pipeline()
            elif self.mode == "traditional":
                success = self.run_traditional_pipeline()
            else:
                self.logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë“œ: {self.mode}")
                return False
            
            # ë³´ê³ ì„œ ìƒì„± ë° ì €ì¥
            report = self.generate_report()
            self.save_report(report)
            
            # ìµœì¢… ê²°ê³¼ ë¡œê¹…
            if success:
                execution_time = time.time() - self.start_time
                self.logger.info("="*60)
                self.logger.info("ğŸ‰ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")
                self.logger.info(f"â° ì´ ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
                self.logger.info("="*60)
            else:
                self.logger.error("âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨")
            
            return success
            
        except Exception as e:
            LoggerUtils.log_error_with_traceback(self.logger, e, "íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
            return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description=f"{PROJECT_NAME} v{VERSION}")
    parser.add_argument('--mode', 
                      choices=['unified', 'traditional'], 
                      default='unified',
                      help='íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ëª¨ë“œ')
    
    args = parser.parse_args()
    
    # íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ìƒì„± ë° ì‹¤í–‰
    manager = PipelineManager(mode=args.mode)
    success = manager.run()
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main() 